{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivPhYdFSgExG"
      },
      "outputs": [],
      "source": [
        "# V4.3 : 3-layer Hierarchy + UMAP + z1z2z3 UMAP + Beta for KLD terms (STL-10)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import umap # For UMAP visualization\n",
        "from sklearn.manifold import TSNE # For t-SNE visualization (optional)\n",
        "import seaborn as sns # For better looking plots\n",
        "\n",
        "# (Your existing imports and Hyper-parameters)\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "image_size =  3 * 96 * 96 # Channels * Height * Width\n",
        "z1_dim = 64\n",
        "z2_dim = 32\n",
        "z3_dim = 16\n",
        "num_epochs = 40 # Increased epochs for better latent space learning\n",
        "batch_size = 128\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Create directories (ensure ./latent_space_plots exists)\n",
        "os.makedirs('./sampled_images_hdvae', exist_ok=True)\n",
        "os.makedirs('./plots_hdvae', exist_ok=True)\n",
        "os.makedirs('./latent_space_plots', exist_ok=True) # New directory for latent space plots\n",
        "\n",
        "# (Your existing dataset loading and HDVAE class definition)\n",
        "# STL-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor() # Puts values in [0, 1]\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.STL10(root='./data', split='train', transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.STL10(root='./data', split='test', transform=transform, download=True)\n",
        "\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# Example for fixed_test_images\n",
        "test_loader_fixed_batch = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=8, shuffle=False)\n",
        "fixed_test_images, _ = next(iter(test_loader_fixed_batch))\n",
        "fixed_test_images = fixed_test_images.to(device)\n",
        "\n",
        "# HDVAE Model - 3 Hierarchical Latent Layers (z1, z2, z3) - Same as before\n",
        "class HDVAE(nn.Module):\n",
        "    def __init__(self, image_size, z1_dim, z2_dim, z3_dim):\n",
        "        super(HDVAE, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.z1_dim = z1_dim\n",
        "        self.z2_dim = z2_dim\n",
        "        self.z3_dim = z3_dim\n",
        "\n",
        "        # Encoder X -> Z1 (Kept current deeper structure)\n",
        "        self.encoder_x_to_z1 = nn.Sequential(\n",
        "            nn.Linear(image_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128)\n",
        "        )\n",
        "        self.fc_mu1 = nn.Linear(128, z1_dim)\n",
        "        self.fc_logvar1 = nn.Linear(128, z1_dim)\n",
        "\n",
        "        # Encoder Z1 -> Z2 (Kept current deeper structure)\n",
        "        self.encoder_z1_to_z2 = nn.Sequential(\n",
        "            nn.Linear(z1_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64)\n",
        "        )\n",
        "        self.fc_mu2 = nn.Linear(64, z2_dim)\n",
        "        self.fc_logvar2 = nn.Linear(64, z2_dim)\n",
        "\n",
        "        # NEW: Encoder Z2 -> Z3\n",
        "        self.encoder_z2_to_z3 = nn.Sequential(\n",
        "            nn.Linear(z2_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64)\n",
        "        )\n",
        "        self.fc_mu3 = nn.Linear(64, z3_dim)\n",
        "        self.fc_logvar3 = nn.Linear(64, z3_dim)\n",
        "\n",
        "        # NEW: Decoder Z3 -> Z2 (Prior for Z2)\n",
        "        self.decoder_z3_to_z2_params = nn.Sequential(\n",
        "            nn.Linear(z3_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128)\n",
        "        )\n",
        "        self.fc_prior_mu2 = nn.Linear(128, z2_dim)\n",
        "        self.fc_prior_logvar2 = nn.Linear(128, z2_dim)\n",
        "\n",
        "        # Decoder Z2 -> Z1 (Prior for Z1 - Kept current deeper structure)\n",
        "        self.decoder_z2_to_z1_params = nn.Sequential(\n",
        "            nn.Linear(z2_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128)\n",
        "        )\n",
        "        self.fc_prior_mu1 = nn.Linear(128, z1_dim)\n",
        "        self.fc_prior_logvar1 = nn.Linear(128, z1_dim)\n",
        "\n",
        "        # Decoder Z1 -> X (Kept current deeper structure)\n",
        "        self.decoder_z1_to_x = nn.Sequential(\n",
        "            nn.Linear(z1_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, image_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = self.encoder_x_to_z1(x.view(-1, self.image_size))\n",
        "        mu1, log_var1 = self.fc_mu1(h1), self.fc_logvar1(h1)\n",
        "        z1 = self.reparameterize(mu1, log_var1)\n",
        "\n",
        "        h2 = self.encoder_z1_to_z2(z1)\n",
        "        mu2, log_var2 = self.fc_mu2(h2), self.fc_logvar2(h2)\n",
        "        z2 = self.reparameterize(mu2, log_var2)\n",
        "\n",
        "        h3 = self.encoder_z2_to_z3(z2)\n",
        "        mu3, log_var3 = self.fc_mu3(h3), self.fc_logvar3(h3)\n",
        "        z3 = self.reparameterize(mu3, log_var3)\n",
        "\n",
        "        return (mu1, log_var1, z1), (mu2, log_var2, z2), (mu3, log_var3, z3)\n",
        "\n",
        "    def decode(self, z1, z2, z3):\n",
        "        x_reconst = self.decoder_z1_to_x(z1)\n",
        "\n",
        "        h_prior_z1 = self.decoder_z2_to_z1_params(z2)\n",
        "        mu_prior_1 = self.fc_prior_mu1(h_prior_z1)\n",
        "        log_var_prior_1 = self.fc_prior_logvar1(h_prior_z1)\n",
        "\n",
        "        h_prior_z2 = self.decoder_z3_to_z2_params(z3)\n",
        "        mu_prior_2 = self.fc_prior_mu2(h_prior_z2)\n",
        "        log_var_prior_2 = self.fc_prior_logvar2(h_prior_z2)\n",
        "\n",
        "        return x_reconst, (mu_prior_1, log_var_prior_1), (mu_prior_2, log_var_prior_2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        (mu1, log_var1, z1), (mu2, log_var2, z2), (mu3, log_var3, z3) = self.encode(x)\n",
        "        x_reconst, (mu_prior_1, log_var_prior_1), (mu_prior_2, log_var_prior_2) = self.decode(z1, z2, z3)\n",
        "\n",
        "        return x_reconst, mu1, log_var1, z1, mu2, log_var2, z2, mu3, log_var3, z3, \\\n",
        "               mu_prior_1, log_var_prior_1, mu_prior_2, log_var_prior_2\n",
        "\n",
        "# Instantiate the model\n",
        "model = HDVAE(image_size, z1_dim, z2_dim, z3_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#print(model)\n",
        "\n",
        "# Loss Function - Same as before\n",
        "def loss_function_hdvae(recon_x, x, mu1, log_var1, mu2, log_var2, mu3, log_var3, mu_prior_1, log_var_prior_1, mu_prior_2, log_var_prior_2,\n",
        "                        beta1=1.0, beta2=1.0, beta3=1.0): # Add beta parameters\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, image_size), reduction='sum')\n",
        "\n",
        "    # Assuming Z3 is the top-most layer, so its prior is N(0,I)\n",
        "    KLD_z3 = -0.5 * torch.sum(1 + log_var3 - mu3.pow(2) - log_var3.exp())\n",
        "\n",
        "    KLD_z2 = 0.5 * torch.sum(\n",
        "        torch.exp(log_var2 - log_var_prior_2) +\n",
        "        ((mu_prior_2 - mu2)**2) * torch.exp(-log_var_prior_2) - 1 +\n",
        "        (log_var_prior_2 - log_var2)\n",
        "    )\n",
        "\n",
        "    KLD_z1 = 0.5 * torch.sum(\n",
        "        torch.exp(log_var1 - log_var_prior_1) +\n",
        "        ((mu_prior_1 - mu1)**2) * torch.exp(-log_var_prior_1) - 1 +\n",
        "        (log_var_prior_1 - log_var1)\n",
        "    )\n",
        "\n",
        "    # Apply beta factors\n",
        "    return BCE + beta1 * KLD_z1 + beta2 * KLD_z2 + beta3 * KLD_z3\n",
        "\n",
        "def calculate_metrics(original_images, reconstructed_images):\n",
        "    # original_np = original_images.detach().cpu().numpy().reshape(-1, 28, 28)\n",
        "    # reconstructed_np = reconstructed_images.detach().cpu().numpy().reshape(-1, 28, 28)\n",
        "    print('mrh = ',original_images.shape)\n",
        "    original_np = original_images.detach().cpu().numpy().reshape(-1, 96, 96)\n",
        "    reconstructed_np = reconstructed_images.detach().cpu().numpy().reshape(-1, 96, 96)\n",
        "    psnr_scores = []\n",
        "    ssim_scores = []\n",
        "    for i in range(original_np.shape[0]):\n",
        "        img_o = original_np[i]\n",
        "        img_r = reconstructed_np[i]\n",
        "        psnr = peak_signal_noise_ratio(img_o, img_r, data_range=1)\n",
        "        psnr_scores.append(psnr)\n",
        "        ssim = structural_similarity(img_o, img_r, data_range=1)\n",
        "        ssim_scores.append(ssim)\n",
        "    return np.mean(psnr_scores), np.mean(ssim_scores)\n",
        "\n",
        "def plot_originals_and_reconstructions(original_images, reconstructed_images, title=\"\", num_display=8):\n",
        "    original_images = original_images.detach().cpu().squeeze()\n",
        "    reconstructed_images = reconstructed_images.detach().cpu().squeeze()\n",
        "\n",
        "    original_display = original_images[:num_display]\n",
        "    reconstructed_display = reconstructed_images[:num_display]\n",
        "\n",
        "    fig, axes = plt.subplots(2, num_display, figsize=(num_display * 1.5, 3))\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[0, i].imshow(original_display[i].numpy(), cmap='gray')\n",
        "        axes[0, i].axis('off')\n",
        "        if i == 0: axes[0, i].set_title(\"Original\", fontsize=8)\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[1, i].imshow(reconstructed_display[i].numpy(), cmap='gray')\n",
        "        axes[1, i].axis('off')\n",
        "        if i == 0: axes[1, i].set_title(\"Reconstructed\", fontsize=8)\n",
        "\n",
        "    plt.suptitle(title, y=1.05, fontsize=10)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 1.0])\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_sampled_images(sampled_images, title=\"\", num_display=8, filename=None):\n",
        "    sampled_images = sampled_images.detach().cpu().squeeze()\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_display, figsize=(num_display * 1.5, 1.5))\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[i].imshow(sampled_images[i].numpy(), cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle(title, y=1.05, fontsize=10)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 1.0])\n",
        "    if filename:\n",
        "        plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_latent_space(latent_coords, labels, title, filename):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.scatterplot(\n",
        "        x=latent_coords[:, 0], y=latent_coords[:, 1],\n",
        "        hue=labels,\n",
        "        palette=sns.color_palette(\"tab10\", 10),\n",
        "        legend=\"full\",\n",
        "        alpha=0.7,\n",
        "        s=15 # Adjust point size\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"UMAP Dimension 1\")\n",
        "    plt.ylabel(\"UMAP Dimension 2\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_all_latent_spaces_together(embeddings, labels, titles): # Removed filename, can save separately\n",
        "    num_plots = len(embeddings)\n",
        "    fig, axes = plt.subplots(1, num_plots, figsize=(num_plots * 8, 8))\n",
        "\n",
        "    # Ensure axes is an array even for single plot case\n",
        "    if num_plots == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i in range(num_plots):\n",
        "        sns.scatterplot(\n",
        "            x=embeddings[i][:, 0], y=embeddings[i][:, 1],\n",
        "            hue=labels,\n",
        "            palette=sns.color_palette(\"tab10\", 10),\n",
        "            legend=\"full\" if i == 0 else False, # Only show legend once\n",
        "            alpha=0.7,\n",
        "            s=15,\n",
        "            ax=axes[i] # Plot on specific subplot\n",
        "        )\n",
        "        axes[i].set_title(titles[i])\n",
        "        axes[i].set_xlabel(\"UMAP Dimension 1\")\n",
        "        axes[i].set_ylabel(\"UMAP Dimension 2\")\n",
        "        axes[i].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(\"./latent_space_plots/\", \"all_umap_plots_combined_by_digit.png\")) # Save for clarity\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# --- NEW FUNCTION: Plotting Z1, Z2, Z3 on one UMAP ---\n",
        "def plot_inter_layer_umap(all_latent_data, layer_labels, filename):\n",
        "    \"\"\"\n",
        "    Plots the UMAP projection of concatenated Z1, Z2, Z3 means,\n",
        "    colored by their originating layer (Z1, Z2, Z3).\n",
        "\n",
        "    Args:\n",
        "        all_latent_data (np.array): Concatenated mu1, mu2, mu3 data.\n",
        "        layer_labels (np.array): Labels indicating origin layer ('Z1', 'Z2', 'Z3').\n",
        "        filename (str): Path to save the plot.\n",
        "    \"\"\"\n",
        "    print(f\"Applying UMAP to combined latent spaces for inter-layer comparison (N={all_latent_data.shape[0]}, D={all_latent_data.shape[1]})...\")\n",
        "\n",
        "    # Initialize UMAP reducer. Adjust n_neighbors and min_dist if needed for different structures.\n",
        "    # n_neighbors=15 (default) works well for balanced local/global structure.\n",
        "    # For denser, more separated clusters, try lower min_dist (e.g., 0.1).\n",
        "    reducer_combined = umap.UMAP(random_state=42)\n",
        "    embedding_combined = reducer_combined.fit_transform(all_latent_data)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.scatterplot(\n",
        "        x=embedding_combined[:, 0], y=embedding_combined[:, 1],\n",
        "        hue=layer_labels,\n",
        "        palette=\"viridis\", # Or \"deep\", \"Paired\", etc. for distinct colors\n",
        "        legend=\"full\",\n",
        "        alpha=0.7,\n",
        "        s=15\n",
        "    )\n",
        "    plt.title(\"UMAP Projection of Z1, Z2, Z3 Latent Spaces (Colored by Layer Origin)\")\n",
        "    plt.xlabel(\"UMAP Dimension 1\")\n",
        "    plt.ylabel(\"UMAP Dimension 2\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"Inter-layer UMAP plot generated: {filename}\")\n",
        "\n",
        "\n",
        "# --- Training Loop (as before) ---\n",
        "# --- Variables to store reconstruction and sampled data for the last epoch ---\n",
        "last_epoch_original_images = None\n",
        "last_epoch_reconstructed_images = None\n",
        "last_epoch_sampled_images = None\n",
        "\n",
        "\n",
        "# Training loop\n",
        "print(\"Starting HDVAE training with 3 hierarchical layers...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, (images, _) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        recon_images, mu1, log_var1, z1, mu2, log_var2, z2, mu3, log_var3, z3, \\\n",
        "               mu_prior_1, log_var_prior_1, mu_prior_2, log_var_prior_2 = model(images)\n",
        "\n",
        "\n",
        "        ### MRH:\n",
        "        ################ linear annealing for Beta  in loss function:\n",
        "        num_annealing_epochs = num_epochs / 2\n",
        "        beta_annealing_factor = min(1.0, epoch / num_annealing_epochs) # e.g., num_annealing_epochs = num_epochs / 2\n",
        "        max_beta1 = 0.5\n",
        "        max_beta2 = 0.2\n",
        "        max_beta3 = 0.1\n",
        "        current_beta1 = beta_annealing_factor * max_beta1\n",
        "        current_beta2 = beta_annealing_factor * max_beta2\n",
        "        current_beta3 = beta_annealing_factor * max_beta3\n",
        "        ################\n",
        "\n",
        "        loss = loss_function_hdvae(recon_images, images,\n",
        "                                   mu1, log_var1,\n",
        "                                   mu2, log_var2,\n",
        "                                   mu3, log_var3,\n",
        "                                   mu_prior_1, log_var_prior_1,\n",
        "                                   mu_prior_2, log_var_prior_2, beta1=current_beta1, beta2=current_beta2, beta3=current_beta3)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "\n",
        "    # --- Evaluation ---\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        recon_images_fixed_current_epoch, _, _, _, _, _, _, _, _, _, _, _, _, _ = model(fixed_test_images)\n",
        "\n",
        "        last_epoch_original_images = fixed_test_images.clone().detach()\n",
        "        # last_epoch_reconstructed_images = recon_images_fixed_current_epoch.view(-1, 1, 28, 28).clone().detach()\n",
        "        last_epoch_reconstructed_images = recon_images_fixed_current_epoch.view(-1, 3, 96, 96).clone().detach()\n",
        "\n",
        "        z3_sample = torch.randn(8, z3_dim).to(device)\n",
        "        h_prior_z2_sample = model.decoder_z3_to_z2_params(z3_sample)\n",
        "        mu_prior_2_sample = model.fc_prior_mu2(h_prior_z2_sample)\n",
        "        log_var_prior_2_sample = model.fc_prior_logvar2(h_prior_z2_sample)\n",
        "        z2_sample = model.reparameterize(mu_prior_2_sample, log_var_prior_2_sample)\n",
        "\n",
        "        h_prior_z1_sample = model.decoder_z2_to_z1_params(z2_sample)\n",
        "        mu_prior_1_sample = model.fc_prior_mu1(h_prior_z1_sample)\n",
        "        log_var_prior_1_sample = model.fc_prior_logvar1(h_prior_z1_sample)\n",
        "        z1_sample = model.reparameterize(mu_prior_1_sample, log_var_prior_1_sample)\n",
        "\n",
        "        # sampled_out = model.decoder_z1_to_x(z1_sample).view(-1, 1, 28, 28)\n",
        "        sampled_out = model.decoder_z1_to_x(z1_sample).view(-1, 3, 96, 96)\n",
        "        last_epoch_sampled_images = sampled_out.clone().detach()\n",
        "\n",
        "        psnr, ssim = calculate_metrics(fixed_test_images, recon_images_fixed_current_epoch)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Avg Loss: {avg_loss:.4f}, PSNR: {psnr:.2f}, SSIM: {ssim:.4f}\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "\n",
        "# --- Post-training Latent Space Visualization (Existing Code) ---\n",
        "print(\"\\nCollecting latent space means for visualization (by digit label)...\")\n",
        "all_mu1s = []\n",
        "all_mu2s = []\n",
        "all_mu3s = []\n",
        "all_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        (mu1, _, _), (mu2, _, _), (mu3, _, _) = model.encode(images)\n",
        "\n",
        "        all_mu1s.append(mu1.cpu().numpy())\n",
        "        all_mu2s.append(mu2.cpu().numpy())\n",
        "        all_mu3s.append(mu3.cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "all_mu1s = np.concatenate(all_mu1s, axis=0)\n",
        "all_mu2s = np.concatenate(all_mu2s, axis=0)\n",
        "all_mu3s = np.concatenate(all_mu3s, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "print(\"Applying UMAP to latent space means (by digit label)...\")\n",
        "\n",
        "reducer_z1 = umap.UMAP(random_state=42)\n",
        "embedding_z1 = reducer_z1.fit_transform(all_mu1s)\n",
        "plot_latent_space(embedding_z1, all_labels,\n",
        "                  \"UMAP Projection of Z1 Latent Space (Higher Resolution)\",\n",
        "                  \"./latent_space_plots/umap_z1_by_digit.png\")\n",
        "\n",
        "reducer_z2 = umap.UMAP(random_state=42)\n",
        "embedding_z2 = reducer_z2.fit_transform(all_mu2s)\n",
        "plot_latent_space(embedding_z2, all_labels,\n",
        "                  \"UMAP Projection of Z2 Latent Space (Medium Resolution)\",\n",
        "                  \"./latent_space_plots/umap_z2_by_digit.png\")\n",
        "\n",
        "reducer_z3 = umap.UMAP(random_state=42)\n",
        "embedding_z3 = reducer_z3.fit_transform(all_mu3s)\n",
        "plot_latent_space(embedding_z3, all_labels,\n",
        "                  \"UMAP Projection of Z3 Latent Space (Lowest Resolution - Most Abstract)\",\n",
        "                  \"./latent_space_plots/umap_z3_by_digit.png\")\n",
        "\n",
        "print(\"Latent space plots (by digit label) generated in './latent_space_plots/' directory.\")\n",
        "\n",
        "# Call the combined plot for digit labels as well\n",
        "plot_all_latent_spaces_together(\n",
        "    [embedding_z1, embedding_z2, embedding_z3],\n",
        "    all_labels,\n",
        "    [\"Z1 Latent Space (by Digit)\", \"Z2 Latent Space (by Digit)\", \"Z3 Latent Space (by Digit)\"]\n",
        ")\n",
        "\n",
        "# --- NEW SECTION: Prepare data for inter-layer UMAP plot ---\n",
        "print(\"\\nPreparing data for inter-layer UMAP plot...\")\n",
        "\n",
        "# 1. Pad/Truncate Latent Vectors to a Common Dimension\n",
        "# Find the maximum dimension\n",
        "max_dim = max(z1_dim, z2_dim, z3_dim)\n",
        "\n",
        "# Function to pad/truncate\n",
        "def adjust_dimension(data, target_dim):\n",
        "    current_dim = data.shape[1]\n",
        "    if current_dim < target_dim:\n",
        "        padding = np.zeros((data.shape[0], target_dim - current_dim))\n",
        "        return np.concatenate((data, padding), axis=1)\n",
        "    elif current_dim > target_dim:\n",
        "        return data[:, :target_dim]\n",
        "    else:\n",
        "        return data\n",
        "\n",
        "# Adjust all mu's to the maximum dimension\n",
        "all_mu1s_adjusted = adjust_dimension(all_mu1s, max_dim)\n",
        "all_mu2s_adjusted = adjust_dimension(all_mu2s, max_dim)\n",
        "all_mu3s_adjusted = adjust_dimension(all_mu3s, max_dim)\n",
        "\n",
        "# 2. Concatenate them\n",
        "# Create labels for each layer\n",
        "layer_labels_z1 = np.full(all_mu1s_adjusted.shape[0], 'Z1')\n",
        "layer_labels_z2 = np.full(all_mu2s_adjusted.shape[0], 'Z2')\n",
        "layer_labels_z3 = np.full(all_mu3s_adjusted.shape[0], 'Z3')\n",
        "\n",
        "# Concatenate the data and the labels\n",
        "all_latent_data_combined = np.concatenate((all_mu1s_adjusted, all_mu2s_adjusted, all_mu3s_adjusted), axis=0)\n",
        "all_layer_labels_combined = np.concatenate((layer_labels_z1, layer_labels_z2, layer_labels_z3), axis=0)\n",
        "\n",
        "# 3. Call the new plotting function\n",
        "plot_inter_layer_umap(all_latent_data_combined, all_layer_labels_combined,\n",
        "                      \"./latent_space_plots/umap_all_layers_combined.png\")\n",
        "\n",
        "# --- Final Display (as before) ---\n",
        "# ... (your existing code for displaying last epoch reconstructions and generated images) ..."
      ]
    }
  ]
}