{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r48yORXk5Y3F"
      },
      "outputs": [],
      "source": [
        "############### Comparison Step ##################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Global History Storage (put these at the top of your notebook) ---\n",
        "# These lists will store the metrics for each epoch for each model.\n",
        "# Ensure 'num_epochs' is defined and consistent across all models.\n",
        "\n",
        "# History for your HDVAE (assuming you have similar variables from its setup)\n",
        "hdvae_psnr_history = []\n",
        "hdvae_ssim_history = []\n",
        "hdvae_loss_history = []\n",
        "\n",
        "# History for Vanilla VAE\n",
        "vanilla_vae_psnr_history = []\n",
        "vanilla_vae_ssim_history = []\n",
        "vanilla_vae_loss_history = []\n",
        "\n",
        "# History for Beta-VAE\n",
        "beta_vae_psnr_history = []\n",
        "beta_vae_ssim_history = []\n",
        "beta_vae_loss_history = []\n",
        "\n",
        "# History for FactorVAE\n",
        "factor_vae_psnr_history = []\n",
        "factor_vae_ssim_history = []\n",
        "factor_vae_loss_history = []\n",
        "\n",
        "# History for DIP-VAE-I\n",
        "dip_vae_psnr_history = []\n",
        "dip_vae_ssim_history = []\n",
        "dip_vae_loss_history = []\n",
        "\n",
        "# Ensure your common parameters are defined, e.g.:\n",
        "# num_epochs = 20 # Or whatever number you're using\n",
        "# image_size = 784\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# fixed_test_images # This should also be defined once and used for all models"
      ],
      "metadata": {
        "id": "cPtXgjx15ZQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Again the same as V4 . the only change is adding       psnr, ssim history\n",
        "# V4: 3-layer Hierarchy + UMAP + z1z2z3 UMAP + Beta for KLD terms (MNIST)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import umap # For UMAP visualization\n",
        "from sklearn.manifold import TSNE # For t-SNE visualization (optional)\n",
        "import seaborn as sns # For better looking plots\n",
        "\n",
        "# (Your existing imports and Hyper-parameters)\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "image_size = 784\n",
        "z1_dim = 64\n",
        "z2_dim = 32\n",
        "z3_dim = 16\n",
        "num_epochs = 20 # Increased epochs for better latent space learning\n",
        "batch_size = 128\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Create directories (ensure ./latent_space_plots exists)\n",
        "os.makedirs('./sampled_images_hdvae', exist_ok=True)\n",
        "os.makedirs('./plots_hdvae', exist_ok=True)\n",
        "os.makedirs('./latent_space_plots', exist_ok=True) # New directory for latent space plots\n",
        "\n",
        "# (Your existing dataset loading and HDVAE class definition)\n",
        "# MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=False)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) # Use batch_size for collecting latents\n",
        "\n",
        "test_loader_fixed_batch = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=8, shuffle=False)\n",
        "fixed_test_images, _ = next(iter(test_loader_fixed_batch))\n",
        "fixed_test_images = fixed_test_images.to(device)\n",
        "\n",
        "\n",
        "# HDVAE Model - 3 Hierarchical Latent Layers (z1, z2, z3) - Same as before\n",
        "class HDVAE(nn.Module):\n",
        "    def __init__(self, image_size, z1_dim, z2_dim, z3_dim):\n",
        "        super(HDVAE, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.z1_dim = z1_dim\n",
        "        self.z2_dim = z2_dim\n",
        "        self.z3_dim = z3_dim\n",
        "\n",
        "        # Encoder X -> Z1 (Kept current deeper structure)\n",
        "        self.encoder_x_to_z1 = nn.Sequential(\n",
        "            nn.Linear(image_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128)\n",
        "        )\n",
        "        self.fc_mu1 = nn.Linear(128, z1_dim)\n",
        "        self.fc_logvar1 = nn.Linear(128, z1_dim)\n",
        "\n",
        "        # Encoder Z1 -> Z2 (Kept current deeper structure)\n",
        "        self.encoder_z1_to_z2 = nn.Sequential(\n",
        "            nn.Linear(z1_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64)\n",
        "        )\n",
        "        self.fc_mu2 = nn.Linear(64, z2_dim)\n",
        "        self.fc_logvar2 = nn.Linear(64, z2_dim)\n",
        "\n",
        "        # NEW: Encoder Z2 -> Z3\n",
        "        self.encoder_z2_to_z3 = nn.Sequential(\n",
        "            nn.Linear(z2_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64)\n",
        "        )\n",
        "        self.fc_mu3 = nn.Linear(64, z3_dim)\n",
        "        self.fc_logvar3 = nn.Linear(64, z3_dim)\n",
        "\n",
        "        # NEW: Decoder Z3 -> Z2 (Prior for Z2)\n",
        "        self.decoder_z3_to_z2_params = nn.Sequential(\n",
        "            nn.Linear(z3_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128)\n",
        "        )\n",
        "        self.fc_prior_mu2 = nn.Linear(128, z2_dim)\n",
        "        self.fc_prior_logvar2 = nn.Linear(128, z2_dim)\n",
        "\n",
        "        # Decoder Z2 -> Z1 (Prior for Z1 - Kept current deeper structure)\n",
        "        self.decoder_z2_to_z1_params = nn.Sequential(\n",
        "            nn.Linear(z2_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128)\n",
        "        )\n",
        "        self.fc_prior_mu1 = nn.Linear(128, z1_dim)\n",
        "        self.fc_prior_logvar1 = nn.Linear(128, z1_dim)\n",
        "\n",
        "        # Decoder Z1 -> X (Kept current deeper structure)\n",
        "        self.decoder_z1_to_x = nn.Sequential(\n",
        "            nn.Linear(z1_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, image_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = self.encoder_x_to_z1(x.view(-1, self.image_size))\n",
        "        mu1, log_var1 = self.fc_mu1(h1), self.fc_logvar1(h1)\n",
        "        z1 = self.reparameterize(mu1, log_var1)\n",
        "\n",
        "        h2 = self.encoder_z1_to_z2(z1)\n",
        "        mu2, log_var2 = self.fc_mu2(h2), self.fc_logvar2(h2)\n",
        "        z2 = self.reparameterize(mu2, log_var2)\n",
        "\n",
        "        h3 = self.encoder_z2_to_z3(z2)\n",
        "        mu3, log_var3 = self.fc_mu3(h3), self.fc_logvar3(h3)\n",
        "        z3 = self.reparameterize(mu3, log_var3)\n",
        "\n",
        "        return (mu1, log_var1, z1), (mu2, log_var2, z2), (mu3, log_var3, z3)\n",
        "\n",
        "    def decode(self, z1, z2, z3):\n",
        "        x_reconst = self.decoder_z1_to_x(z1)\n",
        "\n",
        "        h_prior_z1 = self.decoder_z2_to_z1_params(z2)\n",
        "        mu_prior_1 = self.fc_prior_mu1(h_prior_z1)\n",
        "        log_var_prior_1 = self.fc_prior_logvar1(h_prior_z1)\n",
        "\n",
        "        h_prior_z2 = self.decoder_z3_to_z2_params(z3)\n",
        "        mu_prior_2 = self.fc_prior_mu2(h_prior_z2)\n",
        "        log_var_prior_2 = self.fc_prior_logvar2(h_prior_z2)\n",
        "\n",
        "        return x_reconst, (mu_prior_1, log_var_prior_1), (mu_prior_2, log_var_prior_2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        (mu1, log_var1, z1), (mu2, log_var2, z2), (mu3, log_var3, z3) = self.encode(x)\n",
        "        x_reconst, (mu_prior_1, log_var_prior_1), (mu_prior_2, log_var_prior_2) = self.decode(z1, z2, z3)\n",
        "\n",
        "        return x_reconst, mu1, log_var1, z1, mu2, log_var2, z2, mu3, log_var3, z3, \\\n",
        "               mu_prior_1, log_var_prior_1, mu_prior_2, log_var_prior_2\n",
        "\n",
        "# Instantiate the model\n",
        "model = HDVAE(image_size, z1_dim, z2_dim, z3_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(model)\n",
        "\n",
        "# Loss Function - Same as before\n",
        "def loss_function_hdvae(recon_x, x, mu1, log_var1, mu2, log_var2, mu3, log_var3, mu_prior_1, log_var_prior_1, mu_prior_2, log_var_prior_2,\n",
        "                        beta1=1.0, beta2=1.0, beta3=1.0): # Add beta parameters\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, image_size), reduction='sum')\n",
        "\n",
        "    # Assuming Z3 is the top-most layer, so its prior is N(0,I)\n",
        "    KLD_z3 = -0.5 * torch.sum(1 + log_var3 - mu3.pow(2) - log_var3.exp())\n",
        "\n",
        "    KLD_z2 = 0.5 * torch.sum(\n",
        "        torch.exp(log_var2 - log_var_prior_2) +\n",
        "        ((mu_prior_2 - mu2)**2) * torch.exp(-log_var_prior_2) - 1 +\n",
        "        (log_var_prior_2 - log_var2)\n",
        "    )\n",
        "\n",
        "    KLD_z1 = 0.5 * torch.sum(\n",
        "        torch.exp(log_var1 - log_var_prior_1) +\n",
        "        ((mu_prior_1 - mu1)**2) * torch.exp(-log_var_prior_1) - 1 +\n",
        "        (log_var_prior_1 - log_var1)\n",
        "    )\n",
        "\n",
        "    # Apply beta factors\n",
        "    return BCE + beta1 * KLD_z1 + beta2 * KLD_z2 + beta3 * KLD_z3\n",
        "\n",
        "def calculate_metrics(original_images, reconstructed_images):\n",
        "    original_np = original_images.detach().cpu().numpy().reshape(-1, 28, 28)\n",
        "    reconstructed_np = reconstructed_images.detach().cpu().numpy().reshape(-1, 28, 28)\n",
        "    psnr_scores = []\n",
        "    ssim_scores = []\n",
        "    for i in range(original_np.shape[0]):\n",
        "        img_o = original_np[i]\n",
        "        img_r = reconstructed_np[i]\n",
        "        psnr = peak_signal_noise_ratio(img_o, img_r, data_range=1)\n",
        "        psnr_scores.append(psnr)\n",
        "        ssim = structural_similarity(img_o, img_r, data_range=1)\n",
        "        ssim_scores.append(ssim)\n",
        "    return np.mean(psnr_scores), np.mean(ssim_scores)\n",
        "\n",
        "def plot_originals_and_reconstructions(original_images, reconstructed_images, title=\"\", num_display=8):\n",
        "    original_images = original_images.detach().cpu().squeeze()\n",
        "    reconstructed_images = reconstructed_images.detach().cpu().squeeze()\n",
        "\n",
        "    original_display = original_images[:num_display]\n",
        "    reconstructed_display = reconstructed_images[:num_display]\n",
        "\n",
        "    fig, axes = plt.subplots(2, num_display, figsize=(num_display * 1.5, 3))\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[0, i].imshow(original_display[i].numpy(), cmap='gray')\n",
        "        axes[0, i].axis('off')\n",
        "        if i == 0: axes[0, i].set_title(\"Original\", fontsize=8)\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[1, i].imshow(reconstructed_display[i].numpy(), cmap='gray')\n",
        "        axes[1, i].axis('off')\n",
        "        if i == 0: axes[1, i].set_title(\"Reconstructed\", fontsize=8)\n",
        "\n",
        "    plt.suptitle(title, y=1.05, fontsize=10)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 1.0])\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_sampled_images(sampled_images, title=\"\", num_display=8, filename=None):\n",
        "    sampled_images = sampled_images.detach().cpu().squeeze()\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_display, figsize=(num_display * 1.5, 1.5))\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[i].imshow(sampled_images[i].numpy(), cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle(title, y=1.05, fontsize=10)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 1.0])\n",
        "    if filename:\n",
        "        plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_latent_space(latent_coords, labels, title, filename):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.scatterplot(\n",
        "        x=latent_coords[:, 0], y=latent_coords[:, 1],\n",
        "        hue=labels,\n",
        "        palette=sns.color_palette(\"tab10\", 10),\n",
        "        legend=\"full\",\n",
        "        alpha=0.7,\n",
        "        s=15 # Adjust point size\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"UMAP Dimension 1\")\n",
        "    plt.ylabel(\"UMAP Dimension 2\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_all_latent_spaces_together(embeddings, labels, titles): # Removed filename, can save separately\n",
        "    num_plots = len(embeddings)\n",
        "    fig, axes = plt.subplots(1, num_plots, figsize=(num_plots * 8, 8))\n",
        "\n",
        "    # Ensure axes is an array even for single plot case\n",
        "    if num_plots == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i in range(num_plots):\n",
        "        sns.scatterplot(\n",
        "            x=embeddings[i][:, 0], y=embeddings[i][:, 1],\n",
        "            hue=labels,\n",
        "            palette=sns.color_palette(\"tab10\", 10),\n",
        "            legend=\"full\" if i == 0 else False, # Only show legend once\n",
        "            alpha=0.7,\n",
        "            s=15,\n",
        "            ax=axes[i] # Plot on specific subplot\n",
        "        )\n",
        "        axes[i].set_title(titles[i])\n",
        "        axes[i].set_xlabel(\"UMAP Dimension 1\")\n",
        "        axes[i].set_ylabel(\"UMAP Dimension 2\")\n",
        "        axes[i].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(\"./latent_space_plots/\", \"all_umap_plots_combined_by_digit.png\")) # Save for clarity\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# --- NEW FUNCTION: Plotting Z1, Z2, Z3 on one UMAP ---\n",
        "def plot_inter_layer_umap(all_latent_data, layer_labels, filename):\n",
        "    \"\"\"\n",
        "    Plots the UMAP projection of concatenated Z1, Z2, Z3 means,\n",
        "    colored by their originating layer (Z1, Z2, Z3).\n",
        "\n",
        "    Args:\n",
        "        all_latent_data (np.array): Concatenated mu1, mu2, mu3 data.\n",
        "        layer_labels (np.array): Labels indicating origin layer ('Z1', 'Z2', 'Z3').\n",
        "        filename (str): Path to save the plot.\n",
        "    \"\"\"\n",
        "    print(f\"Applying UMAP to combined latent spaces for inter-layer comparison (N={all_latent_data.shape[0]}, D={all_latent_data.shape[1]})...\")\n",
        "\n",
        "    # Initialize UMAP reducer. Adjust n_neighbors and min_dist if needed for different structures.\n",
        "    # n_neighbors=15 (default) works well for balanced local/global structure.\n",
        "    # For denser, more separated clusters, try lower min_dist (e.g., 0.1).\n",
        "    reducer_combined = umap.UMAP(random_state=42)\n",
        "    embedding_combined = reducer_combined.fit_transform(all_latent_data)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.scatterplot(\n",
        "        x=embedding_combined[:, 0], y=embedding_combined[:, 1],\n",
        "        hue=layer_labels,\n",
        "        palette=\"viridis\", # Or \"deep\", \"Paired\", etc. for distinct colors\n",
        "        legend=\"full\",\n",
        "        alpha=0.7,\n",
        "        s=15\n",
        "    )\n",
        "    plt.title(\"UMAP Projection of Z1, Z2, Z3 Latent Spaces (Colored by Layer Origin)\")\n",
        "    plt.xlabel(\"UMAP Dimension 1\")\n",
        "    plt.ylabel(\"UMAP Dimension 2\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"Inter-layer UMAP plot generated: {filename}\")\n",
        "\n",
        "\n",
        "# --- Training Loop (as before) ---\n",
        "# --- Variables to store reconstruction and sampled data for the last epoch ---\n",
        "last_epoch_original_images = None\n",
        "last_epoch_reconstructed_images = None\n",
        "last_epoch_sampled_images = None\n",
        "\n",
        "\n",
        "# Training loop\n",
        "print(\"Starting HDVAE training with 3 hierarchical layers...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, (images, _) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        recon_images, mu1, log_var1, z1, mu2, log_var2, z2, mu3, log_var3, z3, \\\n",
        "               mu_prior_1, log_var_prior_1, mu_prior_2, log_var_prior_2 = model(images)\n",
        "\n",
        "\n",
        "        ### MRH:\n",
        "        ################ linear annealing for Beta  in loss function:\n",
        "        num_annealing_epochs = num_epochs / 2\n",
        "        beta_annealing_factor = min(1.0, epoch / num_annealing_epochs) # e.g., num_annealing_epochs = num_epochs / 2\n",
        "        max_beta1 = 0.5\n",
        "        max_beta2 = 0.2\n",
        "        max_beta3 = 0.1\n",
        "        current_beta1 = beta_annealing_factor * max_beta1\n",
        "        current_beta2 = beta_annealing_factor * max_beta2\n",
        "        current_beta3 = beta_annealing_factor * max_beta3\n",
        "        ################\n",
        "\n",
        "        loss = loss_function_hdvae(recon_images, images,\n",
        "                                   mu1, log_var1,\n",
        "                                   mu2, log_var2,\n",
        "                                   mu3, log_var3,\n",
        "                                   mu_prior_1, log_var_prior_1,\n",
        "                                   mu_prior_2, log_var_prior_2, beta1=current_beta1, beta2=current_beta2, beta3=current_beta3)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "\n",
        "    # --- Evaluation ---\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        recon_images_fixed_current_epoch, _, _, _, _, _, _, _, _, _, _, _, _, _ = model(fixed_test_images)\n",
        "\n",
        "        last_epoch_original_images = fixed_test_images.clone().detach()\n",
        "        last_epoch_reconstructed_images = recon_images_fixed_current_epoch.view(-1, 1, 28, 28).clone().detach()\n",
        "\n",
        "        z3_sample = torch.randn(8, z3_dim).to(device)\n",
        "        h_prior_z2_sample = model.decoder_z3_to_z2_params(z3_sample)\n",
        "        mu_prior_2_sample = model.fc_prior_mu2(h_prior_z2_sample)\n",
        "        log_var_prior_2_sample = model.fc_prior_logvar2(h_prior_z2_sample)\n",
        "        z2_sample = model.reparameterize(mu_prior_2_sample, log_var_prior_2_sample)\n",
        "\n",
        "        h_prior_z1_sample = model.decoder_z2_to_z1_params(z2_sample)\n",
        "        mu_prior_1_sample = model.fc_prior_mu1(h_prior_z1_sample)\n",
        "        log_var_prior_1_sample = model.fc_prior_logvar1(h_prior_z1_sample)\n",
        "        z1_sample = model.reparameterize(mu_prior_1_sample, log_var_prior_1_sample)\n",
        "\n",
        "        sampled_out = model.decoder_z1_to_x(z1_sample).view(-1, 1, 28, 28)\n",
        "        last_epoch_sampled_images = sampled_out.clone().detach()\n",
        "\n",
        "        psnr, ssim = calculate_metrics(fixed_test_images, recon_images_fixed_current_epoch)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Avg Loss: {avg_loss:.4f}, PSNR: {psnr:.2f}, SSIM: {ssim:.4f}\")\n",
        "\n",
        "        # --- ADD THESE LINES TO YOUR HDVAE TRAINING LOOP ---\n",
        "        hdvae_psnr_history.append(psnr)\n",
        "        hdvae_ssim_history.append(ssim)\n",
        "        hdvae_loss_history.append(avg_loss) # Make sure this is the average training loss per epoch\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "\n",
        "# --- Post-training Latent Space Visualization (Existing Code) ---\n",
        "print(\"\\nCollecting latent space means for visualization (by digit label)...\")\n",
        "all_mu1s = []\n",
        "all_mu2s = []\n",
        "all_mu3s = []\n",
        "all_labels = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        (mu1, _, _), (mu2, _, _), (mu3, _, _) = model.encode(images)\n",
        "\n",
        "        all_mu1s.append(mu1.cpu().numpy())\n",
        "        all_mu2s.append(mu2.cpu().numpy())\n",
        "        all_mu3s.append(mu3.cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "all_mu1s = np.concatenate(all_mu1s, axis=0)\n",
        "all_mu2s = np.concatenate(all_mu2s, axis=0)\n",
        "all_mu3s = np.concatenate(all_mu3s, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "print(\"Applying UMAP to latent space means (by digit label)...\")\n",
        "\n",
        "reducer_z1 = umap.UMAP(random_state=42)\n",
        "embedding_z1 = reducer_z1.fit_transform(all_mu1s)\n",
        "plot_latent_space(embedding_z1, all_labels,\n",
        "                  \"UMAP Projection of Z1 Latent Space (Higher Resolution)\",\n",
        "                  \"./latent_space_plots/umap_z1_by_digit.png\")\n",
        "\n",
        "reducer_z2 = umap.UMAP(random_state=42)\n",
        "embedding_z2 = reducer_z2.fit_transform(all_mu2s)\n",
        "plot_latent_space(embedding_z2, all_labels,\n",
        "                  \"UMAP Projection of Z2 Latent Space (Medium Resolution)\",\n",
        "                  \"./latent_space_plots/umap_z2_by_digit.png\")\n",
        "\n",
        "reducer_z3 = umap.UMAP(random_state=42)\n",
        "embedding_z3 = reducer_z3.fit_transform(all_mu3s)\n",
        "plot_latent_space(embedding_z3, all_labels,\n",
        "                  \"UMAP Projection of Z3 Latent Space (Lowest Resolution - Most Abstract)\",\n",
        "                  \"./latent_space_plots/umap_z3_by_digit.png\")\n",
        "\n",
        "print(\"Latent space plots (by digit label) generated in './latent_space_plots/' directory.\")\n",
        "\n",
        "# Call the combined plot for digit labels as well\n",
        "plot_all_latent_spaces_together(\n",
        "    [embedding_z1, embedding_z2, embedding_z3],\n",
        "    all_labels,\n",
        "    [\"Z1 Latent Space (by Digit)\", \"Z2 Latent Space (by Digit)\", \"Z3 Latent Space (by Digit)\"]\n",
        ")\n",
        "\n",
        "# --- NEW SECTION: Prepare data for inter-layer UMAP plot ---\n",
        "print(\"\\nPreparing data for inter-layer UMAP plot...\")\n",
        "\n",
        "# 1. Pad/Truncate Latent Vectors to a Common Dimension\n",
        "# Find the maximum dimension\n",
        "max_dim = max(z1_dim, z2_dim, z3_dim)\n",
        "\n",
        "# Function to pad/truncate\n",
        "def adjust_dimension(data, target_dim):\n",
        "    current_dim = data.shape[1]\n",
        "    if current_dim < target_dim:\n",
        "        padding = np.zeros((data.shape[0], target_dim - current_dim))\n",
        "        return np.concatenate((data, padding), axis=1)\n",
        "    elif current_dim > target_dim:\n",
        "        return data[:, :target_dim]\n",
        "    else:\n",
        "        return data\n",
        "\n",
        "# Adjust all mu's to the maximum dimension\n",
        "all_mu1s_adjusted = adjust_dimension(all_mu1s, max_dim)\n",
        "all_mu2s_adjusted = adjust_dimension(all_mu2s, max_dim)\n",
        "all_mu3s_adjusted = adjust_dimension(all_mu3s, max_dim)\n",
        "\n",
        "# 2. Concatenate them\n",
        "# Create labels for each layer\n",
        "layer_labels_z1 = np.full(all_mu1s_adjusted.shape[0], 'Z1')\n",
        "layer_labels_z2 = np.full(all_mu2s_adjusted.shape[0], 'Z2')\n",
        "layer_labels_z3 = np.full(all_mu3s_adjusted.shape[0], 'Z3')\n",
        "\n",
        "# Concatenate the data and the labels\n",
        "all_latent_data_combined = np.concatenate((all_mu1s_adjusted, all_mu2s_adjusted, all_mu3s_adjusted), axis=0)\n",
        "all_layer_labels_combined = np.concatenate((layer_labels_z1, layer_labels_z2, layer_labels_z3), axis=0)\n",
        "\n",
        "# 3. Call the new plotting function\n",
        "plot_inter_layer_umap(all_latent_data_combined, all_layer_labels_combined,\n",
        "                      \"./latent_space_plots/umap_all_layers_combined.png\")\n",
        "\n",
        "# --- Final Display (as before) ---\n",
        "# ... (your existing code for displaying last epoch reconstructions and generated images) ..."
      ],
      "metadata": {
        "id": "1yKH5lSU5hnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9j3enG6C5q5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# V4: Vanila VAE for comparison\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import umap # For UMAP visualization\n",
        "from sklearn.manifold import TSNE # For t-SNE visualization (optional)\n",
        "import seaborn as sns # For better looking plots\n",
        "\n",
        "# (Your existing imports and Hyper-parameters)\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "image_size = 784\n",
        "z1_dim = 64\n",
        "z2_dim = 32\n",
        "z3_dim = 16\n",
        "num_epochs = 20 # Increased epochs for better latent space learning\n",
        "batch_size = 128\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Create directories (ensure ./latent_space_plots exists)\n",
        "os.makedirs('./sampled_images_hdvae', exist_ok=True)\n",
        "os.makedirs('./plots_hdvae', exist_ok=True)\n",
        "os.makedirs('./latent_space_plots', exist_ok=True) # New directory for latent space plots\n",
        "\n",
        "# (Your existing dataset loading and HDVAE class definition)\n",
        "# MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=False)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) # Use batch_size for collecting latents\n",
        "\n",
        "test_loader_fixed_batch = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=8, shuffle=False)\n",
        "fixed_test_images, _ = next(iter(test_loader_fixed_batch))\n",
        "fixed_test_images = fixed_test_images.to(device)\n",
        "\n",
        "\n",
        "class VanillaVAE(nn.Module):\n",
        "    def __init__(self, image_size, z_dim):\n",
        "        super(VanillaVAE, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder (similar to HDVAE's encoder_x_to_z1, adjusted for z_dim)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(image_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128)\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(128, z_dim)\n",
        "        self.fc_logvar = nn.Linear(128, z_dim)\n",
        "\n",
        "        # Decoder (similar to HDVAE's decoder_z1_to_x)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, image_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        h = self.encoder(x.view(-1, self.image_size))\n",
        "        mu = self.fc_mu(h)\n",
        "        log_var = self.fc_logvar(h)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "\n",
        "        # Decode\n",
        "        recon_x = self.decoder(z)\n",
        "        return recon_x, mu, log_var\n",
        "\n",
        "# Instantiate the model\n",
        "model = HDVAE(image_size, z1_dim, z2_dim, z3_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# print(model)\n",
        "\n",
        "# --- Loss Function for Vanilla VAE ---\n",
        "def loss_function_vanilla_vae(recon_x, x, mu, log_var, beta=1.0):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, image_size), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "def calculate_metrics(original_images, reconstructed_images):\n",
        "    original_np = original_images.detach().cpu().numpy().reshape(-1, 28, 28)\n",
        "    reconstructed_np = reconstructed_images.detach().cpu().numpy().reshape(-1, 28, 28)\n",
        "    psnr_scores = []\n",
        "    ssim_scores = []\n",
        "    for i in range(original_np.shape[0]):\n",
        "        img_o = original_np[i]\n",
        "        img_r = reconstructed_np[i]\n",
        "        psnr = peak_signal_noise_ratio(img_o, img_r, data_range=1)\n",
        "        psnr_scores.append(psnr)\n",
        "        ssim = structural_similarity(img_o, img_r, data_range=1)\n",
        "        ssim_scores.append(ssim)\n",
        "    return np.mean(psnr_scores), np.mean(ssim_scores)\n",
        "\n",
        "def plot_originals_and_reconstructions(original_images, reconstructed_images, title=\"\", num_display=8):\n",
        "    original_images = original_images.detach().cpu().squeeze()\n",
        "    reconstructed_images = reconstructed_images.detach().cpu().squeeze()\n",
        "\n",
        "    original_display = original_images[:num_display]\n",
        "    reconstructed_display = reconstructed_images[:num_display]\n",
        "\n",
        "    fig, axes = plt.subplots(2, num_display, figsize=(num_display * 1.5, 3))\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[0, i].imshow(original_display[i].numpy(), cmap='gray')\n",
        "        axes[0, i].axis('off')\n",
        "        if i == 0: axes[0, i].set_title(\"Original\", fontsize=8)\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[1, i].imshow(reconstructed_display[i].numpy(), cmap='gray')\n",
        "        axes[1, i].axis('off')\n",
        "        if i == 0: axes[1, i].set_title(\"Reconstructed\", fontsize=8)\n",
        "\n",
        "    plt.suptitle(title, y=1.05, fontsize=10)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 1.0])\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_sampled_images(sampled_images, title=\"\", num_display=8, filename=None):\n",
        "    sampled_images = sampled_images.detach().cpu().squeeze()\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_display, figsize=(num_display * 1.5, 1.5))\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[i].imshow(sampled_images[i].numpy(), cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle(title, y=1.05, fontsize=10)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 1.0])\n",
        "    if filename:\n",
        "        plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_latent_space(latent_coords, labels, title, filename):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.scatterplot(\n",
        "        x=latent_coords[:, 0], y=latent_coords[:, 1],\n",
        "        hue=labels,\n",
        "        palette=sns.color_palette(\"tab10\", 10),\n",
        "        legend=\"full\",\n",
        "        alpha=0.7,\n",
        "        s=15 # Adjust point size\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"UMAP Dimension 1\")\n",
        "    plt.ylabel(\"UMAP Dimension 2\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_all_latent_spaces_together(embeddings, labels, titles): # Removed filename, can save separately\n",
        "    num_plots = len(embeddings)\n",
        "    fig, axes = plt.subplots(1, num_plots, figsize=(num_plots * 8, 8))\n",
        "\n",
        "    # Ensure axes is an array even for single plot case\n",
        "    if num_plots == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i in range(num_plots):\n",
        "        sns.scatterplot(\n",
        "            x=embeddings[i][:, 0], y=embeddings[i][:, 1],\n",
        "            hue=labels,\n",
        "            palette=sns.color_palette(\"tab10\", 10),\n",
        "            legend=\"full\" if i == 0 else False, # Only show legend once\n",
        "            alpha=0.7,\n",
        "            s=15,\n",
        "            ax=axes[i] # Plot on specific subplot\n",
        "        )\n",
        "        axes[i].set_title(titles[i])\n",
        "        axes[i].set_xlabel(\"UMAP Dimension 1\")\n",
        "        axes[i].set_ylabel(\"UMAP Dimension 2\")\n",
        "        axes[i].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(\"./latent_space_plots/\", \"all_umap_plots_combined_by_digit.png\")) # Save for clarity\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# --- NEW FUNCTION: Plotting Z1, Z2, Z3 on one UMAP ---\n",
        "def plot_inter_layer_umap(all_latent_data, layer_labels, filename):\n",
        "    \"\"\"\n",
        "    Plots the UMAP projection of concatenated Z1, Z2, Z3 means,\n",
        "    colored by their originating layer (Z1, Z2, Z3).\n",
        "\n",
        "    Args:\n",
        "        all_latent_data (np.array): Concatenated mu1, mu2, mu3 data.\n",
        "        layer_labels (np.array): Labels indicating origin layer ('Z1', 'Z2', 'Z3').\n",
        "        filename (str): Path to save the plot.\n",
        "    \"\"\"\n",
        "    print(f\"Applying UMAP to combined latent spaces for inter-layer comparison (N={all_latent_data.shape[0]}, D={all_latent_data.shape[1]})...\")\n",
        "\n",
        "    # Initialize UMAP reducer. Adjust n_neighbors and min_dist if needed for different structures.\n",
        "    # n_neighbors=15 (default) works well for balanced local/global structure.\n",
        "    # For denser, more separated clusters, try lower min_dist (e.g., 0.1).\n",
        "    reducer_combined = umap.UMAP(random_state=42)\n",
        "    embedding_combined = reducer_combined.fit_transform(all_latent_data)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.scatterplot(\n",
        "        x=embedding_combined[:, 0], y=embedding_combined[:, 1],\n",
        "        hue=layer_labels,\n",
        "        palette=\"viridis\", # Or \"deep\", \"Paired\", etc. for distinct colors\n",
        "        legend=\"full\",\n",
        "        alpha=0.7,\n",
        "        s=15\n",
        "    )\n",
        "    plt.title(\"UMAP Projection of Z1, Z2, Z3 Latent Spaces (Colored by Layer Origin)\")\n",
        "    plt.xlabel(\"UMAP Dimension 1\")\n",
        "    plt.ylabel(\"UMAP Dimension 2\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"Inter-layer UMAP plot generated: {filename}\")\n",
        "\n",
        "\n",
        "# --- Training Loop (as before) ---\n",
        "# --- Variables to store reconstruction and sampled data for the last epoch ---\n",
        "last_epoch_original_images = None\n",
        "last_epoch_reconstructed_images = None\n",
        "last_epoch_sampled_images = None\n",
        "\n",
        "\n",
        "vanilla_z_dim = z1_dim # Let's use 64 for a direct comparison with the first layer of HDVAE\n",
        "\n",
        "vanilla_vae_model = VanillaVAE(image_size, vanilla_z_dim).to(device)\n",
        "vanilla_vae_optimizer = torch.optim.Adam(vanilla_vae_model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"\\n--- Vanilla VAE Model ---\")\n",
        "# print(vanilla_vae_model)\n",
        "\n",
        "# --- Training Loop for Vanilla VAE ---\n",
        "print(\"\\nStarting Vanilla VAE training...\")\n",
        "for epoch in range(num_epochs): # Using the same num_epochs as HDVAE for direct comparison\n",
        "    vanilla_vae_model.train()\n",
        "    total_loss = 0\n",
        "    for i, (images, _) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        recon_images, mu, log_var = vanilla_vae_model(images)\n",
        "\n",
        "        # You can also apply beta annealing here if you want to be consistent with HDVAE\n",
        "        # current_beta = min(1.0, epoch / (num_epochs / 2)) # Example simple linear annealing\n",
        "        # loss = loss_function_vanilla_vae(recon_images, images, mu, log_var, beta=current_beta)\n",
        "\n",
        "        loss = loss_function_vanilla_vae(recon_images, images, mu, log_var) # Using beta=1.0 by default\n",
        "\n",
        "        vanilla_vae_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        vanilla_vae_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f\"Vanilla VAE - Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "\n",
        "    # --- Vanilla VAE Evaluation (similar to HDVAE evaluation) ---\n",
        "    vanilla_vae_model.eval()\n",
        "    with torch.no_grad():\n",
        "        recon_images_fixed_current_epoch_vanilla, _, _ = vanilla_vae_model(fixed_test_images)\n",
        "\n",
        "        # Sampling from Vanilla VAE\n",
        "        z_sample_vanilla = torch.randn(8, vanilla_z_dim).to(device)\n",
        "        sampled_out_vanilla = vanilla_vae_model.decoder(z_sample_vanilla).view(-1, 1, 28, 28)\n",
        "\n",
        "        psnr_vanilla, ssim_vanilla = calculate_metrics(fixed_test_images, recon_images_fixed_current_epoch_vanilla)\n",
        "\n",
        "        print(f\"Vanilla VAE - Epoch [{epoch+1}/{num_epochs}] completed. Avg Loss: {avg_loss:.4f}, PSNR: {psnr_vanilla:.2f}, SSIM: {ssim_vanilla:.4f}\")\n",
        "\n",
        "        # --- ADD THESE LINES ---\n",
        "        vanilla_vae_psnr_history.append(psnr_vanilla)\n",
        "        vanilla_vae_ssim_history.append(ssim_vanilla)\n",
        "        vanilla_vae_loss_history.append(avg_loss)\n",
        "\n",
        "# --- Post-training Visualization for Vanilla VAE ---\n",
        "print(\"\\nCollecting Vanilla VAE latent space means for visualization (by digit label)...\")\n",
        "all_vanilla_mus = []\n",
        "all_vanilla_labels = []\n",
        "\n",
        "vanilla_vae_model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        _, mu, _ = vanilla_vae_model(images) # Only need mu for UMAP\n",
        "\n",
        "        all_vanilla_mus.append(mu.cpu().numpy())\n",
        "        all_vanilla_labels.append(labels.cpu().numpy())\n",
        "\n",
        "all_vanilla_mus = np.concatenate(all_vanilla_mus, axis=0)\n",
        "all_vanilla_labels = np.concatenate(all_vanilla_labels, axis=0)\n",
        "\n",
        "print(\"Applying UMAP to Vanilla VAE latent space means (by digit label)...\")\n",
        "reducer_vanilla_vae = umap.UMAP(random_state=42)\n",
        "embedding_vanilla_vae = reducer_vanilla_vae.fit_transform(all_vanilla_mus)\n",
        "plot_latent_space(embedding_vanilla_vae, all_vanilla_labels,\n",
        "                  \"UMAP Projection of Vanilla VAE Latent Space\",\n",
        "                  \"./latent_space_plots/umap_vanilla_vae_by_digit.png\")\n",
        "\n",
        "print(\"Vanilla VAE latent space plot generated in './latent_space_plots/' directory.\")\n"
      ],
      "metadata": {
        "id": "hns4la3h5rd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yyZEsEYE5sIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# V4: Beta-VAE for comparison\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import umap # For UMAP visualization\n",
        "from sklearn.manifold import TSNE # For t-SNE visualization (optional)\n",
        "import seaborn as sns # For better looking plots\n",
        "\n",
        "# (Your existing imports and Hyper-parameters)\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "image_size = 784\n",
        "z1_dim = 64\n",
        "z2_dim = 32\n",
        "z3_dim = 16\n",
        "num_epochs = 20 # Increased epochs for better latent space learning\n",
        "batch_size = 128\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Create directories (ensure ./latent_space_plots exists)\n",
        "os.makedirs('./sampled_images_hdvae', exist_ok=True)\n",
        "os.makedirs('./plots_hdvae', exist_ok=True)\n",
        "os.makedirs('./latent_space_plots', exist_ok=True) # New directory for latent space plots\n",
        "\n",
        "# (Your existing dataset loading and HDVAE class definition)\n",
        "# MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=False)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) # Use batch_size for collecting latents\n",
        "\n",
        "test_loader_fixed_batch = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=8, shuffle=False)\n",
        "fixed_test_images, _ = next(iter(test_loader_fixed_batch))\n",
        "fixed_test_images = fixed_test_images.to(device)\n",
        "\n",
        "\n",
        "class BetaVAE(nn.Module):\n",
        "    def __init__(self, image_size, z_dim):\n",
        "        super(BetaVAE, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder (identical to Vanilla VAE and HDVAE's encoder_x_to_z1 part)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(image_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128)\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(128, z_dim)\n",
        "        self.fc_logvar = nn.Linear(128, z_dim)\n",
        "\n",
        "        # Decoder (identical to Vanilla VAE and HDVAE's decoder_z1_to_x part)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, image_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        h = self.encoder(x.view(-1, self.image_size))\n",
        "        mu = self.fc_mu(h)\n",
        "        log_var = self.fc_logvar(h)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "\n",
        "        # Decode\n",
        "        recon_x = self.decoder(z)\n",
        "        return recon_x, mu, log_var\n",
        "\n",
        "# Instantiate the model\n",
        "model = HDVAE(image_size, z1_dim, z2_dim, z3_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#print(model)\n",
        "\n",
        "# --- Loss Function for Vanilla VAE ---\n",
        "def loss_function_beta_vae(recon_x, x, mu, log_var, beta): # beta is now a required argument\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, image_size), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    return BCE + beta * KLD\n",
        "\n",
        "def calculate_metrics(original_images, reconstructed_images):\n",
        "    original_np = original_images.detach().cpu().numpy().reshape(-1, 28, 28)\n",
        "    reconstructed_np = reconstructed_images.detach().cpu().numpy().reshape(-1, 28, 28)\n",
        "    psnr_scores = []\n",
        "    ssim_scores = []\n",
        "    for i in range(original_np.shape[0]):\n",
        "        img_o = original_np[i]\n",
        "        img_r = reconstructed_np[i]\n",
        "        psnr = peak_signal_noise_ratio(img_o, img_r, data_range=1)\n",
        "        psnr_scores.append(psnr)\n",
        "        ssim = structural_similarity(img_o, img_r, data_range=1)\n",
        "        ssim_scores.append(ssim)\n",
        "    return np.mean(psnr_scores), np.mean(ssim_scores)\n",
        "\n",
        "def plot_originals_and_reconstructions(original_images, reconstructed_images, title=\"\", num_display=8):\n",
        "    original_images = original_images.detach().cpu().squeeze()\n",
        "    reconstructed_images = reconstructed_images.detach().cpu().squeeze()\n",
        "\n",
        "    original_display = original_images[:num_display]\n",
        "    reconstructed_display = reconstructed_images[:num_display]\n",
        "\n",
        "    fig, axes = plt.subplots(2, num_display, figsize=(num_display * 1.5, 3))\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[0, i].imshow(original_display[i].numpy(), cmap='gray')\n",
        "        axes[0, i].axis('off')\n",
        "        if i == 0: axes[0, i].set_title(\"Original\", fontsize=8)\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[1, i].imshow(reconstructed_display[i].numpy(), cmap='gray')\n",
        "        axes[1, i].axis('off')\n",
        "        if i == 0: axes[1, i].set_title(\"Reconstructed\", fontsize=8)\n",
        "\n",
        "    plt.suptitle(title, y=1.05, fontsize=10)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 1.0])\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_sampled_images(sampled_images, title=\"\", num_display=8, filename=None):\n",
        "    sampled_images = sampled_images.detach().cpu().squeeze()\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_display, figsize=(num_display * 1.5, 1.5))\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[i].imshow(sampled_images[i].numpy(), cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle(title, y=1.05, fontsize=10)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 1.0])\n",
        "    if filename:\n",
        "        plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_latent_space(latent_coords, labels, title, filename):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.scatterplot(\n",
        "        x=latent_coords[:, 0], y=latent_coords[:, 1],\n",
        "        hue=labels,\n",
        "        palette=sns.color_palette(\"tab10\", 10),\n",
        "        legend=\"full\",\n",
        "        alpha=0.7,\n",
        "        s=15 # Adjust point size\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"UMAP Dimension 1\")\n",
        "    plt.ylabel(\"UMAP Dimension 2\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_all_latent_spaces_together(embeddings, labels, titles): # Removed filename, can save separately\n",
        "    num_plots = len(embeddings)\n",
        "    fig, axes = plt.subplots(1, num_plots, figsize=(num_plots * 8, 8))\n",
        "\n",
        "    # Ensure axes is an array even for single plot case\n",
        "    if num_plots == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i in range(num_plots):\n",
        "        sns.scatterplot(\n",
        "            x=embeddings[i][:, 0], y=embeddings[i][:, 1],\n",
        "            hue=labels,\n",
        "            palette=sns.color_palette(\"tab10\", 10),\n",
        "            legend=\"full\" if i == 0 else False, # Only show legend once\n",
        "            alpha=0.7,\n",
        "            s=15,\n",
        "            ax=axes[i] # Plot on specific subplot\n",
        "        )\n",
        "        axes[i].set_title(titles[i])\n",
        "        axes[i].set_xlabel(\"UMAP Dimension 1\")\n",
        "        axes[i].set_ylabel(\"UMAP Dimension 2\")\n",
        "        axes[i].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(\"./latent_space_plots/\", \"all_umap_plots_combined_by_digit.png\")) # Save for clarity\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# --- NEW FUNCTION: Plotting Z1, Z2, Z3 on one UMAP ---\n",
        "def plot_inter_layer_umap(all_latent_data, layer_labels, filename):\n",
        "    \"\"\"\n",
        "    Plots the UMAP projection of concatenated Z1, Z2, Z3 means,\n",
        "    colored by their originating layer (Z1, Z2, Z3).\n",
        "\n",
        "    Args:\n",
        "        all_latent_data (np.array): Concatenated mu1, mu2, mu3 data.\n",
        "        layer_labels (np.array): Labels indicating origin layer ('Z1', 'Z2', 'Z3').\n",
        "        filename (str): Path to save the plot.\n",
        "    \"\"\"\n",
        "    print(f\"Applying UMAP to combined latent spaces for inter-layer comparison (N={all_latent_data.shape[0]}, D={all_latent_data.shape[1]})...\")\n",
        "\n",
        "    # Initialize UMAP reducer. Adjust n_neighbors and min_dist if needed for different structures.\n",
        "    # n_neighbors=15 (default) works well for balanced local/global structure.\n",
        "    # For denser, more separated clusters, try lower min_dist (e.g., 0.1).\n",
        "    reducer_combined = umap.UMAP(random_state=42)\n",
        "    embedding_combined = reducer_combined.fit_transform(all_latent_data)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.scatterplot(\n",
        "        x=embedding_combined[:, 0], y=embedding_combined[:, 1],\n",
        "        hue=layer_labels,\n",
        "        palette=\"viridis\", # Or \"deep\", \"Paired\", etc. for distinct colors\n",
        "        legend=\"full\",\n",
        "        alpha=0.7,\n",
        "        s=15\n",
        "    )\n",
        "    plt.title(\"UMAP Projection of Z1, Z2, Z3 Latent Spaces (Colored by Layer Origin)\")\n",
        "    plt.xlabel(\"UMAP Dimension 1\")\n",
        "    plt.ylabel(\"UMAP Dimension 2\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"Inter-layer UMAP plot generated: {filename}\")\n",
        "\n",
        "\n",
        "# --- Training Loop (as before) ---\n",
        "# --- Variables to store reconstruction and sampled data for the last epoch ---\n",
        "last_epoch_original_images = None\n",
        "last_epoch_reconstructed_images = None\n",
        "last_epoch_sampled_images = None\n",
        "\n",
        "\n",
        "beta_vae_z_dim = z1_dim # Still 64\n",
        "\n",
        "# Choose a beta value. Common values are 4 or 8 to encourage disentanglement.\n",
        "# Let's start with 4.0. You can experiment with this.\n",
        "beta_vae_beta_value = 4.0\n",
        "\n",
        "beta_vae_model = BetaVAE(image_size, beta_vae_z_dim).to(device)\n",
        "beta_vae_optimizer = torch.optim.Adam(beta_vae_model.parameters(), lr=learning_rate)\n",
        "\n",
        "#print(\"\\n--- Beta-VAE Model ---\")\n",
        "#print(beta_vae_model)\n",
        "\n",
        "# --- Training Loop for Beta-VAE ---\n",
        "print(f\"\\nStarting Beta-VAE training with beta = {beta_vae_beta_value}...\")\n",
        "for epoch in range(num_epochs): # Using the same num_epochs as HDVAE for direct comparison\n",
        "    beta_vae_model.train()\n",
        "    total_loss = 0\n",
        "    for i, (images, _) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        recon_images, mu, log_var = beta_vae_model(images)\n",
        "\n",
        "        # Here we pass the fixed beta_vae_beta_value\n",
        "        loss = loss_function_beta_vae(recon_images, images, mu, log_var, beta=beta_vae_beta_value)\n",
        "\n",
        "        beta_vae_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        beta_vae_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f\"Beta-VAE - Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "\n",
        "    # --- Beta-VAE Evaluation (similar to HDVAE evaluation) ---\n",
        "    beta_vae_model.eval()\n",
        "    with torch.no_grad():\n",
        "        recon_images_fixed_current_epoch_beta_vae, _, _ = beta_vae_model(fixed_test_images)\n",
        "\n",
        "        # Sampling from Beta-VAE\n",
        "        z_sample_beta_vae = torch.randn(8, beta_vae_z_dim).to(device)\n",
        "        sampled_out_beta_vae = beta_vae_model.decoder(z_sample_beta_vae).view(-1, 1, 28, 28)\n",
        "\n",
        "        psnr_beta_vae, ssim_beta_vae = calculate_metrics(fixed_test_images, recon_images_fixed_current_epoch_beta_vae)\n",
        "\n",
        "        print(f\"Beta-VAE - Epoch [{epoch+1}/{num_epochs}] completed. Avg Loss: {avg_loss:.4f}, PSNR: {psnr_beta_vae:.2f}, SSIM: {ssim_beta_vae:.4f}\")\n",
        "\n",
        "        # --- ADD THESE LINES ---\n",
        "        beta_vae_psnr_history.append(psnr_beta_vae)\n",
        "        beta_vae_ssim_history.append(ssim_beta_vae)\n",
        "        beta_vae_loss_history.append(avg_loss)\n",
        "\n",
        "# --- Post-training Visualization for Beta-VAE ---\n",
        "print(\"\\nCollecting Beta-VAE latent space means for visualization (by digit label)...\")\n",
        "all_beta_vae_mus = []\n",
        "all_beta_vae_labels = []\n",
        "\n",
        "beta_vae_model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        _, mu, _ = beta_vae_model(images) # Only need mu for UMAP\n",
        "\n",
        "        all_beta_vae_mus.append(mu.cpu().numpy())\n",
        "        all_beta_vae_labels.append(labels.cpu().numpy())\n",
        "\n",
        "all_beta_vae_mus = np.concatenate(all_beta_vae_mus, axis=0)\n",
        "all_beta_vae_labels = np.concatenate(all_beta_vae_labels, axis=0)\n",
        "\n",
        "print(\"Applying UMAP to Beta-VAE latent space means (by digit label)...\")\n",
        "reducer_beta_vae = umap.UMAP(random_state=42)\n",
        "embedding_beta_vae = reducer_beta_vae.fit_transform(all_beta_vae_mus)\n",
        "plot_latent_space(embedding_beta_vae, all_beta_vae_labels,\n",
        "                  f\"UMAP Projection of Beta-VAE Latent Space (Beta={beta_vae_beta_value})\",\n",
        "                  \"./latent_space_plots/umap_beta_vae_by_digit.png\")\n",
        "\n",
        "print(\"Beta-VAE latent space plot generated in './latent_space_plots/' directory.\")"
      ],
      "metadata": {
        "id": "9CfLS7U55sOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JAkxa3R55vNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# V4: FactorVAE for comparison\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import umap # For UMAP visualization\n",
        "from sklearn.manifold import TSNE # For t-SNE visualization (optional)\n",
        "import seaborn as sns # For better looking plots\n",
        "\n",
        "# (Your existing imports and Hyper-parameters)\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "image_size = 784\n",
        "z1_dim = 64\n",
        "z2_dim = 32\n",
        "z3_dim = 16\n",
        "num_epochs = 20 # Increased epochs for better latent space learning\n",
        "batch_size = 128\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Create directories (ensure ./latent_space_plots exists)\n",
        "os.makedirs('./sampled_images_hdvae', exist_ok=True)\n",
        "os.makedirs('./plots_hdvae', exist_ok=True)\n",
        "os.makedirs('./latent_space_plots', exist_ok=True) # New directory for latent space plots\n",
        "\n",
        "# (Your existing dataset loading and HDVAE class definition)\n",
        "# MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=False)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) # Use batch_size for collecting latents\n",
        "\n",
        "test_loader_fixed_batch = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=8, shuffle=False)\n",
        "fixed_test_images, _ = next(iter(test_loader_fixed_batch))\n",
        "fixed_test_images = fixed_test_images.to(device)\n",
        "\n",
        "\n",
        "# --- FactorVAE Model ---\n",
        "# FactorVAE aims for disentanglement by adding a Total Correlation (TC) penalty.\n",
        "\n",
        "class FactorVAE(nn.Module):\n",
        "    def __init__(self, image_size, z_dim):\n",
        "        super(FactorVAE, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder (identical to Vanilla/Beta VAE)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(image_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128)\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(128, z_dim)\n",
        "        self.fc_logvar = nn.Linear(128, z_dim)\n",
        "\n",
        "        # Decoder (identical to Vanilla/Beta VAE)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, image_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        h = self.encoder(x.view(-1, self.image_size))\n",
        "        mu = self.fc_mu(h)\n",
        "        log_var = self.fc_logvar(h)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "\n",
        "        # Decode\n",
        "        recon_x = self.decoder(z)\n",
        "        return recon_x, mu, log_var, z # Return z for TC calculation\n",
        "\n",
        "# --- Discriminator for FactorVAE ---\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, z_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(z_dim, 1000),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1000, 1000),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1000, 1000),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1000, 1000),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1000, 1000),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1000, 1000),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(1000, 2) # Output logits for binary classification (real vs. permuted)\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)\n",
        "\n",
        "# Instantiate the model\n",
        "model = HDVAE(image_size, z1_dim, z2_dim, z3_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#print(model)\n",
        "\n",
        "# --- FactorVAE Loss Function Components ---\n",
        "# The loss calculation is more complex and will be handled within the training loop.\n",
        "# Here's the KL divergence part, standard for VAEs.\n",
        "def kl_divergence(mu, log_var):\n",
        "    return -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "def calculate_metrics(original_images, reconstructed_images):\n",
        "    original_np = original_images.detach().cpu().numpy().reshape(-1, 28, 28)\n",
        "    reconstructed_np = reconstructed_images.detach().cpu().numpy().reshape(-1, 28, 28)\n",
        "    psnr_scores = []\n",
        "    ssim_scores = []\n",
        "    for i in range(original_np.shape[0]):\n",
        "        img_o = original_np[i]\n",
        "        img_r = reconstructed_np[i]\n",
        "        psnr = peak_signal_noise_ratio(img_o, img_r, data_range=1)\n",
        "        psnr_scores.append(psnr)\n",
        "        ssim = structural_similarity(img_o, img_r, data_range=1)\n",
        "        ssim_scores.append(ssim)\n",
        "    return np.mean(psnr_scores), np.mean(ssim_scores)\n",
        "\n",
        "def plot_originals_and_reconstructions(original_images, reconstructed_images, title=\"\", num_display=8):\n",
        "    original_images = original_images.detach().cpu().squeeze()\n",
        "    reconstructed_images = reconstructed_images.detach().cpu().squeeze()\n",
        "\n",
        "    original_display = original_images[:num_display]\n",
        "    reconstructed_display = reconstructed_images[:num_display]\n",
        "\n",
        "    fig, axes = plt.subplots(2, num_display, figsize=(num_display * 1.5, 3))\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[0, i].imshow(original_display[i].numpy(), cmap='gray')\n",
        "        axes[0, i].axis('off')\n",
        "        if i == 0: axes[0, i].set_title(\"Original\", fontsize=8)\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[1, i].imshow(reconstructed_display[i].numpy(), cmap='gray')\n",
        "        axes[1, i].axis('off')\n",
        "        if i == 0: axes[1, i].set_title(\"Reconstructed\", fontsize=8)\n",
        "\n",
        "    plt.suptitle(title, y=1.05, fontsize=10)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 1.0])\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_sampled_images(sampled_images, title=\"\", num_display=8, filename=None):\n",
        "    sampled_images = sampled_images.detach().cpu().squeeze()\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_display, figsize=(num_display * 1.5, 1.5))\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[i].imshow(sampled_images[i].numpy(), cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle(title, y=1.05, fontsize=10)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 1.0])\n",
        "    if filename:\n",
        "        plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_latent_space(latent_coords, labels, title, filename):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.scatterplot(\n",
        "        x=latent_coords[:, 0], y=latent_coords[:, 1],\n",
        "        hue=labels,\n",
        "        palette=sns.color_palette(\"tab10\", 10),\n",
        "        legend=\"full\",\n",
        "        alpha=0.7,\n",
        "        s=15 # Adjust point size\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"UMAP Dimension 1\")\n",
        "    plt.ylabel(\"UMAP Dimension 2\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_all_latent_spaces_together(embeddings, labels, titles): # Removed filename, can save separately\n",
        "    num_plots = len(embeddings)\n",
        "    fig, axes = plt.subplots(1, num_plots, figsize=(num_plots * 8, 8))\n",
        "\n",
        "    # Ensure axes is an array even for single plot case\n",
        "    if num_plots == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i in range(num_plots):\n",
        "        sns.scatterplot(\n",
        "            x=embeddings[i][:, 0], y=embeddings[i][:, 1],\n",
        "            hue=labels,\n",
        "            palette=sns.color_palette(\"tab10\", 10),\n",
        "            legend=\"full\" if i == 0 else False, # Only show legend once\n",
        "            alpha=0.7,\n",
        "            s=15,\n",
        "            ax=axes[i] # Plot on specific subplot\n",
        "        )\n",
        "        axes[i].set_title(titles[i])\n",
        "        axes[i].set_xlabel(\"UMAP Dimension 1\")\n",
        "        axes[i].set_ylabel(\"UMAP Dimension 2\")\n",
        "        axes[i].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(\"./latent_space_plots/\", \"all_umap_plots_combined_by_digit.png\")) # Save for clarity\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# --- NEW FUNCTION: Plotting Z1, Z2, Z3 on one UMAP ---\n",
        "def plot_inter_layer_umap(all_latent_data, layer_labels, filename):\n",
        "    \"\"\"\n",
        "    Plots the UMAP projection of concatenated Z1, Z2, Z3 means,\n",
        "    colored by their originating layer (Z1, Z2, Z3).\n",
        "\n",
        "    Args:\n",
        "        all_latent_data (np.array): Concatenated mu1, mu2, mu3 data.\n",
        "        layer_labels (np.array): Labels indicating origin layer ('Z1', 'Z2', 'Z3').\n",
        "        filename (str): Path to save the plot.\n",
        "    \"\"\"\n",
        "    print(f\"Applying UMAP to combined latent spaces for inter-layer comparison (N={all_latent_data.shape[0]}, D={all_latent_data.shape[1]})...\")\n",
        "\n",
        "    # Initialize UMAP reducer. Adjust n_neighbors and min_dist if needed for different structures.\n",
        "    # n_neighbors=15 (default) works well for balanced local/global structure.\n",
        "    # For denser, more separated clusters, try lower min_dist (e.g., 0.1).\n",
        "    reducer_combined = umap.UMAP(random_state=42)\n",
        "    embedding_combined = reducer_combined.fit_transform(all_latent_data)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.scatterplot(\n",
        "        x=embedding_combined[:, 0], y=embedding_combined[:, 1],\n",
        "        hue=layer_labels,\n",
        "        palette=\"viridis\", # Or \"deep\", \"Paired\", etc. for distinct colors\n",
        "        legend=\"full\",\n",
        "        alpha=0.7,\n",
        "        s=15\n",
        "    )\n",
        "    plt.title(\"UMAP Projection of Z1, Z2, Z3 Latent Spaces (Colored by Layer Origin)\")\n",
        "    plt.xlabel(\"UMAP Dimension 1\")\n",
        "    plt.ylabel(\"UMAP Dimension 2\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"Inter-layer UMAP plot generated: {filename}\")\n",
        "\n",
        "\n",
        "# --- Training Loop (as before) ---\n",
        "# --- Variables to store reconstruction and sampled data for the last epoch ---\n",
        "last_epoch_original_images = None\n",
        "last_epoch_reconstructed_images = None\n",
        "last_epoch_sampled_images = None\n",
        "\n",
        "\n",
        "# --- Setup for FactorVAE Training ---\n",
        "factor_vae_z_dim = z1_dim # Still 64 for consistency\n",
        "# Hyperparameter gamma for Total Correlation loss\n",
        "factor_vae_gamma = 10.0 # Common values range from 1 to 100\n",
        "# Discriminator training iterations per VAE training iteration\n",
        "num_discriminator_steps = 1 # Usually 1 or 2\n",
        "\n",
        "factor_vae_model = FactorVAE(image_size, factor_vae_z_dim).to(device)\n",
        "discriminator_model = Discriminator(factor_vae_z_dim).to(device)\n",
        "\n",
        "factor_vae_optimizer = torch.optim.Adam(factor_vae_model.parameters(), lr=learning_rate)\n",
        "discriminator_optimizer = torch.optim.Adam(discriminator_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Binary Cross-Entropy Loss for the discriminator\n",
        "bce_logits = nn.CrossEntropyLoss() # For D, 2 output logits\n",
        "\n",
        "print(\"\\n--- FactorVAE Model ---\")\n",
        "print(factor_vae_model)\n",
        "print(\"\\n--- FactorVAE Discriminator Model ---\")\n",
        "print(discriminator_model)\n",
        "\n",
        "# --- Training Loop for FactorVAE ---\n",
        "print(f\"\\nStarting FactorVAE training with gamma = {factor_vae_gamma}...\")\n",
        "for epoch in range(num_epochs):\n",
        "    factor_vae_model.train()\n",
        "    discriminator_model.train()\n",
        "    total_vae_loss = 0\n",
        "    total_disc_loss = 0\n",
        "\n",
        "    for i, (images, _) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        batch_size = images.size(0)\n",
        "\n",
        "        # --- Train Discriminator ---\n",
        "        for _ in range(num_discriminator_steps):\n",
        "            discriminator_optimizer.zero_grad()\n",
        "\n",
        "            recon_images, mu, log_var, z = factor_vae_model(images) # Get z for discriminator\n",
        "\n",
        "            # Sample z from aggregate posterior q(z) - approximated by current batch z's\n",
        "            # Create a permuted copy of z\n",
        "            z_permuted = []\n",
        "            for _z in z.transpose(0, 1): # Iterate over each latent dimension\n",
        "                idx = torch.randperm(batch_size) # Random permutation for each dim\n",
        "                z_permuted.append(_z[idx])\n",
        "            z_permuted = torch.stack(z_permuted, 1) # Stack back to (batch_size, z_dim)\n",
        "\n",
        "            # Pass through discriminator\n",
        "            d_z = discriminator_model(z.detach()) # Detach z from VAE graph\n",
        "            d_z_permuted = discriminator_model(z_permuted.detach()) # Detach z_permuted\n",
        "\n",
        "            # Discriminator loss: try to classify real vs. permuted\n",
        "            # Real samples should be classified as 0, permuted as 1 (or vice-versa depending on setup)\n",
        "            real_labels = torch.zeros(batch_size).long().to(device) # Label 0 for original\n",
        "            perm_labels = torch.ones(batch_size).long().to(device)  # Label 1 for permuted\n",
        "\n",
        "            disc_loss = 0.5 * (bce_logits(d_z, real_labels) + bce_logits(d_z_permuted, perm_labels))\n",
        "\n",
        "            disc_loss.backward()\n",
        "            discriminator_optimizer.step()\n",
        "            total_disc_loss += disc_loss.item()\n",
        "\n",
        "        # --- Train VAE ---\n",
        "        factor_vae_optimizer.zero_grad()\n",
        "\n",
        "        recon_images, mu, log_var, z = factor_vae_model(images)\n",
        "\n",
        "        # 1. Reconstruction Loss\n",
        "        recon_loss = F.binary_cross_entropy(recon_images, images.view(-1, image_size), reduction='sum')\n",
        "\n",
        "        # 2. KL Divergence (Standard VAE KLD)\n",
        "        kld_loss = kl_divergence(mu, log_var)\n",
        "\n",
        "        # 3. Total Correlation (TC) Loss\n",
        "        # This term tries to fool the discriminator into thinking real samples are permuted.\n",
        "        # It's log(D(z)) - log(1-D(z))\n",
        "        d_z = discriminator_model(z) # No detach here, we want gradients to flow to VAE\n",
        "\n",
        "        # Log-probabilities for TC calculation\n",
        "        # The discriminator outputs 2 logits: [logit_real, logit_permuted]\n",
        "        # P(z is real) = softmax(d_z)[0], P(z is permuted) = softmax(d_z)[1]\n",
        "        # We want log(P(z is real) / P(z is permuted))\n",
        "\n",
        "        # Safe way to compute log(P_real / P_permuted) or similar quantities\n",
        "        # using log_softmax and then subtraction\n",
        "        logits_real = d_z[:, 0]\n",
        "        logits_perm = d_z[:, 1]\n",
        "\n",
        "        # TC loss is derived from the discriminator's ability to distinguish real from permuted.\n",
        "        # It's (log(D(z)_real) - log(D(z)_permuted)) for the VAE's loss,\n",
        "        # encouraging D(z)_real to be small and D(z)_permuted to be large.\n",
        "        # This is essentially making D(z) predict permuted for real samples.\n",
        "        tc_loss = (logits_real - logits_perm).mean() # As per FactorVAE paper's interpretation\n",
        "\n",
        "        # Full VAE Loss\n",
        "        vae_loss = recon_loss + kld_loss + factor_vae_gamma * tc_loss\n",
        "\n",
        "        vae_loss.backward()\n",
        "        factor_vae_optimizer.step()\n",
        "        total_vae_loss += vae_loss.item()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f\"FactorVAE - Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], VAE Loss: {vae_loss.item():.4f}, Disc Loss: {disc_loss.item():.4f}\")\n",
        "\n",
        "    avg_vae_loss = total_vae_loss / len(train_loader.dataset)\n",
        "    avg_disc_loss = total_disc_loss / (len(train_loader.dataset) * num_discriminator_steps)\n",
        "\n",
        "\n",
        "    # --- FactorVAE Evaluation ---\n",
        "    factor_vae_model.eval()\n",
        "    with torch.no_grad():\n",
        "        recon_images_fixed_current_epoch_factor_vae, _, _, _ = factor_vae_model(fixed_test_images)\n",
        "\n",
        "        # Sampling from FactorVAE\n",
        "        z_sample_factor_vae = torch.randn(8, factor_vae_z_dim).to(device)\n",
        "        sampled_out_factor_vae = factor_vae_model.decoder(z_sample_factor_vae).view(-1, 1, 28, 28)\n",
        "\n",
        "        psnr_factor_vae, ssim_factor_vae = calculate_metrics(fixed_test_images, recon_images_fixed_current_epoch_factor_vae)\n",
        "\n",
        "        print(f\"FactorVAE - Epoch [{epoch+1}/{num_epochs}] completed. Avg VAE Loss: {avg_vae_loss:.4f}, PSNR: {psnr_factor_vae:.2f}, SSIM: {ssim_factor_vae:.4f}\")\n",
        "\n",
        "        # --- ADD THESE LINES ---\n",
        "        factor_vae_psnr_history.append(psnr_factor_vae)\n",
        "        factor_vae_ssim_history.append(ssim_factor_vae)\n",
        "        factor_vae_loss_history.append(avg_vae_loss) # Store the average VAE loss\n",
        "\n",
        "# --- Post-training Visualization for FactorVAE ---\n",
        "print(\"\\nCollecting FactorVAE latent space means for visualization (by digit label)...\")\n",
        "all_factor_vae_mus = []\n",
        "all_factor_vae_labels = []\n",
        "\n",
        "factor_vae_model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        _, mu, _, _ = factor_vae_model(images) # Get mu for UMAP\n",
        "\n",
        "        all_factor_vae_mus.append(mu.cpu().numpy())\n",
        "        all_factor_vae_labels.append(labels.cpu().numpy())\n",
        "\n",
        "all_factor_vae_mus = np.concatenate(all_factor_vae_mus, axis=0)\n",
        "all_factor_vae_labels = np.concatenate(all_factor_vae_labels, axis=0)\n",
        "\n",
        "print(\"Applying UMAP to FactorVAE latent space means (by digit label)...\")\n",
        "reducer_factor_vae = umap.UMAP(random_state=42)\n",
        "embedding_factor_vae = reducer_factor_vae.fit_transform(all_factor_vae_mus)\n",
        "plot_latent_space(embedding_factor_vae, all_factor_vae_labels,\n",
        "                  f\"UMAP Projection of FactorVAE Latent Space (gamma={factor_vae_gamma})\",\n",
        "                  \"./latent_space_plots/umap_factor_vae_by_digit.png\")\n"
      ],
      "metadata": {
        "id": "ywDufsah5vPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lhr-uBIA5xar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# V4: DIP-VAE-I (Disentangled Inferred Prior VAE, Type I) for comparison\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import umap # For UMAP visualization\n",
        "from sklearn.manifold import TSNE # For t-SNE visualization (optional)\n",
        "import seaborn as sns # For better looking plots\n",
        "\n",
        "# (Your existing imports and Hyper-parameters)\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "image_size = 784\n",
        "z1_dim = 64\n",
        "z2_dim = 32\n",
        "z3_dim = 16\n",
        "num_epochs = 20 # Increased epochs for better latent space learning\n",
        "batch_size = 128\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# Create directories (ensure ./latent_space_plots exists)\n",
        "os.makedirs('./sampled_images_hdvae', exist_ok=True)\n",
        "os.makedirs('./plots_hdvae', exist_ok=True)\n",
        "os.makedirs('./latent_space_plots', exist_ok=True) # New directory for latent space plots\n",
        "\n",
        "# (Your existing dataset loading and HDVAE class definition)\n",
        "# MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=False)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False) # Use batch_size for collecting latents\n",
        "\n",
        "test_loader_fixed_batch = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=8, shuffle=False)\n",
        "fixed_test_images, _ = next(iter(test_loader_fixed_batch))\n",
        "fixed_test_images = fixed_test_images.to(device)\n",
        "\n",
        "\n",
        "# --- DIP-VAE-I Model ---\n",
        "# This model is structurally identical to the Vanilla VAE,\n",
        "# but its loss function includes a regularization term on the covariance of the latent space.\n",
        "\n",
        "class DIPVAE(nn.Module):\n",
        "    def __init__(self, image_size, z_dim):\n",
        "        super(DIPVAE, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        # Encoder (identical to previous VAEs)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(image_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128)\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(128, z_dim)\n",
        "        self.fc_logvar = nn.Linear(128, z_dim)\n",
        "\n",
        "        # Decoder (identical to previous VAEs)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, image_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        h = self.encoder(x.view(-1, self.image_size))\n",
        "        mu = self.fc_mu(h)\n",
        "        log_var = self.fc_logvar(h)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "\n",
        "        # Decode\n",
        "        recon_x = self.decoder(z)\n",
        "        return recon_x, mu, log_var, z # Return z for DIP-VAE regularization\n",
        "\n",
        "# Instantiate the model\n",
        "model = HDVAE(image_size, z1_dim, z2_dim, z3_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#print(model)\n",
        "\n",
        "# --- DIP-VAE-I Loss Function (adapted to calculate covariance within the loop) ---\n",
        "# The total loss will be calculated in the training loop.\n",
        "\n",
        "# Function to calculate empirical covariance for a batch of latent vectors\n",
        "def batch_covariance(z):\n",
        "    batch_size = z.size(0)\n",
        "    z_centered = z - z.mean(dim=0, keepdim=True)\n",
        "    covariance = torch.matmul(z_centered.transpose(0, 1), z_centered) / (batch_size - 1)\n",
        "    return covariance\n",
        "\n",
        "\n",
        "\n",
        "def calculate_metrics(original_images, reconstructed_images):\n",
        "    original_np = original_images.detach().cpu().numpy().reshape(-1, 28, 28)\n",
        "    reconstructed_np = reconstructed_images.detach().cpu().numpy().reshape(-1, 28, 28)\n",
        "    psnr_scores = []\n",
        "    ssim_scores = []\n",
        "    for i in range(original_np.shape[0]):\n",
        "        img_o = original_np[i]\n",
        "        img_r = reconstructed_np[i]\n",
        "        psnr = peak_signal_noise_ratio(img_o, img_r, data_range=1)\n",
        "        psnr_scores.append(psnr)\n",
        "        ssim = structural_similarity(img_o, img_r, data_range=1)\n",
        "        ssim_scores.append(ssim)\n",
        "    return np.mean(psnr_scores), np.mean(ssim_scores)\n",
        "\n",
        "def plot_originals_and_reconstructions(original_images, reconstructed_images, title=\"\", num_display=8):\n",
        "    original_images = original_images.detach().cpu().squeeze()\n",
        "    reconstructed_images = reconstructed_images.detach().cpu().squeeze()\n",
        "\n",
        "    original_display = original_images[:num_display]\n",
        "    reconstructed_display = reconstructed_images[:num_display]\n",
        "\n",
        "    fig, axes = plt.subplots(2, num_display, figsize=(num_display * 1.5, 3))\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[0, i].imshow(original_display[i].numpy(), cmap='gray')\n",
        "        axes[0, i].axis('off')\n",
        "        if i == 0: axes[0, i].set_title(\"Original\", fontsize=8)\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[1, i].imshow(reconstructed_display[i].numpy(), cmap='gray')\n",
        "        axes[1, i].axis('off')\n",
        "        if i == 0: axes[1, i].set_title(\"Reconstructed\", fontsize=8)\n",
        "\n",
        "    plt.suptitle(title, y=1.05, fontsize=10)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 1.0])\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_sampled_images(sampled_images, title=\"\", num_display=8, filename=None):\n",
        "    sampled_images = sampled_images.detach().cpu().squeeze()\n",
        "\n",
        "    fig, axes = plt.subplots(1, num_display, figsize=(num_display * 1.5, 1.5))\n",
        "\n",
        "    for i in range(num_display):\n",
        "        axes[i].imshow(sampled_images[i].numpy(), cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle(title, y=1.05, fontsize=10)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 1.0])\n",
        "    if filename:\n",
        "        plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_latent_space(latent_coords, labels, title, filename):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.scatterplot(\n",
        "        x=latent_coords[:, 0], y=latent_coords[:, 1],\n",
        "        hue=labels,\n",
        "        palette=sns.color_palette(\"tab10\", 10),\n",
        "        legend=\"full\",\n",
        "        alpha=0.7,\n",
        "        s=15 # Adjust point size\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"UMAP Dimension 1\")\n",
        "    plt.ylabel(\"UMAP Dimension 2\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def plot_all_latent_spaces_together(embeddings, labels, titles): # Removed filename, can save separately\n",
        "    num_plots = len(embeddings)\n",
        "    fig, axes = plt.subplots(1, num_plots, figsize=(num_plots * 8, 8))\n",
        "\n",
        "    # Ensure axes is an array even for single plot case\n",
        "    if num_plots == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i in range(num_plots):\n",
        "        sns.scatterplot(\n",
        "            x=embeddings[i][:, 0], y=embeddings[i][:, 1],\n",
        "            hue=labels,\n",
        "            palette=sns.color_palette(\"tab10\", 10),\n",
        "            legend=\"full\" if i == 0 else False, # Only show legend once\n",
        "            alpha=0.7,\n",
        "            s=15,\n",
        "            ax=axes[i] # Plot on specific subplot\n",
        "        )\n",
        "        axes[i].set_title(titles[i])\n",
        "        axes[i].set_xlabel(\"UMAP Dimension 1\")\n",
        "        axes[i].set_ylabel(\"UMAP Dimension 2\")\n",
        "        axes[i].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(\"./latent_space_plots/\", \"all_umap_plots_combined_by_digit.png\")) # Save for clarity\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# --- NEW FUNCTION: Plotting Z1, Z2, Z3 on one UMAP ---\n",
        "def plot_inter_layer_umap(all_latent_data, layer_labels, filename):\n",
        "    \"\"\"\n",
        "    Plots the UMAP projection of concatenated Z1, Z2, Z3 means,\n",
        "    colored by their originating layer (Z1, Z2, Z3).\n",
        "\n",
        "    Args:\n",
        "        all_latent_data (np.array): Concatenated mu1, mu2, mu3 data.\n",
        "        layer_labels (np.array): Labels indicating origin layer ('Z1', 'Z2', 'Z3').\n",
        "        filename (str): Path to save the plot.\n",
        "    \"\"\"\n",
        "    print(f\"Applying UMAP to combined latent spaces for inter-layer comparison (N={all_latent_data.shape[0]}, D={all_latent_data.shape[1]})...\")\n",
        "\n",
        "    # Initialize UMAP reducer. Adjust n_neighbors and min_dist if needed for different structures.\n",
        "    # n_neighbors=15 (default) works well for balanced local/global structure.\n",
        "    # For denser, more separated clusters, try lower min_dist (e.g., 0.1).\n",
        "    reducer_combined = umap.UMAP(random_state=42)\n",
        "    embedding_combined = reducer_combined.fit_transform(all_latent_data)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.scatterplot(\n",
        "        x=embedding_combined[:, 0], y=embedding_combined[:, 1],\n",
        "        hue=layer_labels,\n",
        "        palette=\"viridis\", # Or \"deep\", \"Paired\", etc. for distinct colors\n",
        "        legend=\"full\",\n",
        "        alpha=0.7,\n",
        "        s=15\n",
        "    )\n",
        "    plt.title(\"UMAP Projection of Z1, Z2, Z3 Latent Spaces (Colored by Layer Origin)\")\n",
        "    plt.xlabel(\"UMAP Dimension 1\")\n",
        "    plt.ylabel(\"UMAP Dimension 2\")\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(f\"Inter-layer UMAP plot generated: {filename}\")\n",
        "\n",
        "\n",
        "# --- Training Loop (as before) ---\n",
        "# --- Variables to store reconstruction and sampled data for the last epoch ---\n",
        "last_epoch_original_images = None\n",
        "last_epoch_reconstructed_images = None\n",
        "last_epoch_sampled_images = None\n",
        "\n",
        "\n",
        "# --- Setup for DIP-VAE-I Training ---\n",
        "dip_vae_z_dim = z1_dim # Still 64 for consistency\n",
        "# Hyperparameter lambda for DIP-VAE regularization\n",
        "dip_vae_lambda = 100.0 # Common values range from 1 to 200 for MNIST\n",
        "# Hyperparameter beta for standard KLD (we'll keep it 1.0 for simplicity, like vanilla)\n",
        "dip_vae_beta = 1.0\n",
        "\n",
        "dip_vae_model = DIPVAE(image_size, dip_vae_z_dim).to(device)\n",
        "dip_vae_optimizer = torch.optim.Adam(dip_vae_model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"\\n--- DIP-VAE-I Model ---\")\n",
        "print(dip_vae_model)\n",
        "\n",
        "# --- Training Loop for DIP-VAE-I ---\n",
        "print(f\"\\nStarting DIP-VAE-I training with lambda = {dip_vae_lambda}...\")\n",
        "for epoch in range(num_epochs):\n",
        "    dip_vae_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (images, _) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        batch_size = images.size(0)\n",
        "\n",
        "        recon_images, mu, log_var, z = dip_vae_model(images) # Get z for regularization\n",
        "\n",
        "        # 1. Reconstruction Loss\n",
        "        bce_loss = F.binary_cross_entropy(recon_images, images.view(-1, image_size), reduction='sum')\n",
        "\n",
        "        # 2. KL Divergence\n",
        "        kld_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "        # 3. DIP-VAE-I Regularization Term\n",
        "        # Calculate empirical covariance of z for the current batch\n",
        "        z_cov = batch_covariance(z)\n",
        "\n",
        "        # Create an identity matrix of size z_dim x z_dim\n",
        "        identity_matrix = torch.eye(dip_vae_z_dim).to(device)\n",
        "\n",
        "        # Calculate Frobenius norm squared of (Cov(z) - I)\n",
        "        dip_loss = torch.norm(z_cov - identity_matrix, p='fro')**2\n",
        "\n",
        "        # Total Loss\n",
        "        loss = bce_loss + dip_vae_beta * kld_loss + dip_vae_lambda * dip_loss\n",
        "\n",
        "        dip_vae_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        dip_vae_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f\"DIP-VAE-I - Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "\n",
        "    # --- DIP-VAE-I Evaluation ---\n",
        "    dip_vae_model.eval()\n",
        "    with torch.no_grad():\n",
        "        recon_images_fixed_current_epoch_dip_vae, _, _, _ = dip_vae_model(fixed_test_images)\n",
        "\n",
        "        # Sampling from DIP-VAE-I\n",
        "        z_sample_dip_vae = torch.randn(8, dip_vae_z_dim).to(device)\n",
        "        sampled_out_dip_vae = dip_vae_model.decoder(z_sample_dip_vae).view(-1, 1, 28, 28)\n",
        "\n",
        "        psnr_dip_vae, ssim_dip_vae = calculate_metrics(fixed_test_images, recon_images_fixed_current_epoch_dip_vae)\n",
        "\n",
        "        print(f\"DIP-VAE-I - Epoch [{epoch+1}/{num_epochs}] completed. Avg Loss: {avg_loss:.4f}, PSNR: {psnr_dip_vae:.2f}, SSIM: {ssim_dip_vae:.4f}\")\n",
        "\n",
        "        # --- ADD THESE LINES ---\n",
        "\n",
        "        dip_vae_psnr_history.append(psnr_dip_vae)\n",
        "        dip_vae_ssim_history.append(ssim_dip_vae)\n",
        "        dip_vae_loss_history.append(avg_loss)\n",
        "\n",
        "# --- Post-training Visualization for DIP-VAE-I ---\n",
        "print(\"\\nCollecting DIP-VAE-I latent space means for visualization (by digit label)...\")\n",
        "all_dip_vae_mus = []\n",
        "all_dip_vae_labels = []\n",
        "\n",
        "dip_vae_model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        _, mu, _, _ = dip_vae_model(images) # Get mu for UMAP\n",
        "\n",
        "        all_dip_vae_mus.append(mu.cpu().numpy())\n",
        "        all_dip_vae_labels.append(labels.cpu().numpy())\n",
        "\n",
        "all_dip_vae_mus = np.concatenate(all_dip_vae_mus, axis=0)\n",
        "all_dip_vae_labels = np.concatenate(all_dip_vae_labels, axis=0)\n",
        "\n",
        "print(\"Applying UMAP to DIP-VAE-I latent space means (by digit label)...\")\n",
        "reducer_dip_vae = umap.UMAP(random_state=42)\n",
        "embedding_dip_vae = reducer_dip_vae.fit_transform(all_dip_vae_mus)\n",
        "plot_latent_space(embedding_dip_vae, all_dip_vae_labels,\n",
        "                  f\"UMAP Projection of DIP-VAE-I Latent Space (lambda={dip_vae_lambda})\",\n",
        "                  \"./latent_space_plots/umap_dip_vae_by_digit.png\")\n",
        "\n",
        "print(\"DIP-VAE-I latent space plot generated in './latent_space_plots/' directory.\")"
      ],
      "metadata": {
        "id": "B0kR4bWp5xdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NNo-NEy054lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# --- Call the plotting function ---\n",
        "# Ensure all_model_metrics and num_epochs are defined and populated.\n",
        "# Calculate max_epochs_achieved (as done previously)\n",
        "max_epochs_achieved = 0\n",
        "for model_name, data in all_model_metrics.items():\n",
        "    if data[\"psnr\"]:\n",
        "        max_epochs_achieved = max(max_epochs_achieved, len(data[\"psnr\"]))\n",
        "    if data[\"ssim\"]:\n",
        "        max_epochs_achieved = max(max_epochs_achieved, len(data[\"ssim\"]))\n",
        "    if data[\"loss\"]:\n",
        "        max_epochs_achieved = max(max_epochs_achieved, len(data[\"loss\"]))\n",
        "\n",
        "\n",
        "def plot_comparative_metrics_robust(metrics_data, max_epochs_for_x_axis, plot_dir=\"./comparison_plots_robust/\"):\n",
        "    os.makedirs(plot_dir, exist_ok=True)\n",
        "\n",
        "    # Define a default line width\n",
        "    line_thickness = 2.5 # You can adjust this value (e.g., 1.5, 2, 3)\n",
        "\n",
        "    # Plot PSNR\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for model_name, data in metrics_data.items():\n",
        "        if data[\"psnr\"] and len(data[\"psnr\"]) > 0:\n",
        "            current_epochs = range(1, len(data[\"psnr\"]) + 1)\n",
        "            plt.plot(current_epochs, data[\"psnr\"], label=model_name, linewidth=line_thickness) # <--- ADDED linewidth\n",
        "    plt.title(\"PSNR over Epochs for Different VAE Models\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"PSNR (dB)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xlim(1, max_epochs_for_x_axis)\n",
        "    plt.savefig(os.path.join(plot_dir, \"comparative_psnr.png\"))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # Plot SSIM\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for model_name, data in metrics_data.items():\n",
        "        if data[\"ssim\"] and len(data[\"ssim\"]) > 0:\n",
        "            current_epochs = range(1, len(data[\"ssim\"]) + 1)\n",
        "            plt.plot(current_epochs, data[\"ssim\"], label=model_name, linewidth=line_thickness) # <--- ADDED linewidth\n",
        "    plt.title(\"SSIM over Epochs for Different VAE Models\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"SSIM\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xlim(1, max_epochs_for_x_axis)\n",
        "    plt.savefig(os.path.join(plot_dir, \"comparative_ssim.png\"))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for model_name, data in metrics_data.items():\n",
        "        if data[\"loss\"] and len(data[\"loss\"]) > 0:\n",
        "            current_epochs = range(1, len(data[\"loss\"]) + 1)\n",
        "            plt.plot(current_epochs, data[\"loss\"], label=model_name, linewidth=line_thickness) # <--- ADDED linewidth\n",
        "    plt.title(\"Average Training Loss over Epochs for Different VAE Models\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xlim(1, max_epochs_for_x_axis)\n",
        "    plt.savefig(os.path.join(plot_dir, \"comparative_loss.png\"))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# Call the robust plotting function\n",
        "plot_comparative_metrics_robust(all_model_metrics, max_epochs_achieved)\n",
        "\n",
        "print(\"\\nComparative plots for PSNR, SSIM, and Loss saved to './comparison_plots_robust/' directory and displayed inline.\")"
      ],
      "metadata": {
        "id": "gannrYxi54sq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
